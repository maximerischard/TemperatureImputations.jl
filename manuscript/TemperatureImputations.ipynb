{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#Illustration-of-bias-induces-by-measurement-hour\" data-toc-modified-id=\"Illustration-of-bias-induces-by-measurement-hour-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Illustration of bias induces by measurement hour</a></div><div class=\"lev2 toc-item\"><a href=\"#Proposed-solution\" data-toc-modified-id=\"Proposed-solution-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Proposed solution</a></div><div class=\"lev1 toc-item\"><a href=\"#First-Spatiotemporal-Model\" data-toc-modified-id=\"First-Spatiotemporal-Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>First Spatiotemporal Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Fitting-the-spatiotemporal-model\" data-toc-modified-id=\"Fitting-the-spatiotemporal-model-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Fitting the spatiotemporal model</a></div><div class=\"lev1 toc-item\"><a href=\"#Predictions-using-nearby-data\" data-toc-modified-id=\"Predictions-using-nearby-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predictions using nearby data</a></div><div class=\"lev1 toc-item\"><a href=\"#Imputations\" data-toc-modified-id=\"Imputations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Imputations</a></div><div class=\"lev2 toc-item\"><a href=\"#Imputing-by-Conditioning-on-Extrema\" data-toc-modified-id=\"Imputing-by-Conditioning-on-Extrema-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Imputing by Conditioning on Extrema</a></div><div class=\"lev2 toc-item\"><a href=\"#Illustration-of-Hamiltonian-Monte-Carlo-with-Smoothmax-Approximation\" data-toc-modified-id=\"Illustration-of-Hamiltonian-Monte-Carlo-with-Smoothmax-Approximation-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Illustration of Hamiltonian Monte Carlo with Smoothmax Approximation</a></div><div class=\"lev2 toc-item\"><a href=\"#Smoothmax-Temperature-Model\" data-toc-modified-id=\"Smoothmax-Temperature-Model-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Smoothmax Temperature Model</a></div><div class=\"lev1 toc-item\"><a href=\"#Model-diagnostics\" data-toc-modified-id=\"Model-diagnostics-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model diagnostics</a></div><div class=\"lev2 toc-item\"><a href=\"#Variogram\" data-toc-modified-id=\"Variogram-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Variogram</a></div><div class=\"lev2 toc-item\"><a href=\"#Error-and-expected-error\" data-toc-modified-id=\"Error-and-expected-error-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Error and expected error</a></div><div class=\"lev1 toc-item\"><a href=\"#Improving-model\" data-toc-modified-id=\"Improving-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Improving model</a></div><div class=\"lev1 toc-item\"><a href=\"#Imputed-summary-statistics\" data-toc-modified-id=\"Imputed-summary-statistics-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Imputed summary statistics</a></div><div class=\"lev1 toc-item\"><a href=\"#Inference-on-measurement-hour\" data-toc-modified-id=\"Inference-on-measurement-hour-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Inference on measurement hour</a></div><div class=\"lev1 toc-item\"><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusion</a></div><div class=\"lev1 toc-item\"><a href=\"#Stan-programs\" data-toc-modified-id=\"Stan-programs-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Stan programs</a></div><div class=\"lev2 toc-item\"><a href=\"#Smoothmax-toy-example\" data-toc-modified-id=\"Smoothmax-toy-example-101\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Smoothmax toy example</a></div><div class=\"lev3 toc-item\"><a href=\"#Without-smoothmax-Approximation\" data-toc-modified-id=\"Without-smoothmax-Approximation-1011\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>Without <code>smoothmax</code> Approximation</a></div><div class=\"lev3 toc-item\"><a href=\"#With-smoothmax-Approximation\" data-toc-modified-id=\"With-smoothmax-Approximation-1012\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;</span>With <code>smoothmax</code> Approximation</a></div><div class=\"lev2 toc-item\"><a href=\"#Temperature-imputations\" data-toc-modified-id=\"Temperature-imputations-102\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Temperature imputations</a></div><div class=\"lev1 toc-item\"><a href=\"#Derivation-of-the-analytic-posterior-for-toy-example\" data-toc-modified-id=\"Derivation-of-the-analytic-posterior-for-toy-example-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Derivation of the analytic posterior for toy example</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\require{cancel}\n",
    "    \\newcommand{\\genericdel}[3]{%\n",
    "      \\left#1#3\\right#2\n",
    "    }\n",
    "    \\newcommand{\\del}[1]{\\genericdel(){#1}}\n",
    "    \\newcommand{\\sbr}[1]{\\genericdel[]{#1}}\n",
    "    \\newcommand{\\cbr}[1]{\\genericdel\\{\\}{#1}}\n",
    "    \\newcommand{\\abs}[1]{\\genericdel||{#1}}\n",
    "    \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "    \\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "    \\DeclareMathOperator{\\Pr}{\\mathbb{p}}\n",
    "    \\DeclareMathOperator{\\E}{\\mathbb{E}}\n",
    "    \\DeclareMathOperator{\\Ind}{\\mathbb{I}}\n",
    "    \\DeclareMathOperator{\\var}{Var}\n",
    "    \\DeclareMathOperator{\\mse}{MSE}\n",
    "    \\DeclareMathOperator{\\cov}{Cov}\n",
    "    \\DeclareMathOperator{\\invchi}{\\mathrm{Inv-\\chi}^2}\n",
    "    \\newcommand{\\effect}{\\mathrm{eff}}\n",
    "    \\newcommand{\\xtilde}{\\widetilde{X}}\n",
    "    \\DeclareMathOperator{\\normal}{\\mathcal{N}}\n",
    "    \\DeclareMathOperator{\\unif}{Uniform}\n",
    "    \\DeclareMathOperator{\\GP}{\\mathcal{GP}}\n",
    "    \\newcommand{\\T}{\\mathrm{T}}\n",
    "    \\newcommand{\\Tn}{\\T_{n}}\n",
    "    \\newcommand{\\Tx}{\\T_{x}}\n",
    "    \\newcommand{\\station}[1]{\\mathrm{station}\\sbr{#1}}\n",
    "    \\newcommand{\\xvec}{\\mathbf{x}}\n",
    "    \\newcommand{\\hvec}{\\mathbf{h}}\n",
    "    \\newcommand{\\indep}{\\perp}\n",
    "    \\newcommand{\\iid}{iid}\n",
    "    \\newcommand{\\trans}{^{\\intercal}}\n",
    "    \\newcommand{\\sigmaf}{\\sigma_{\\mathrm{GP}}}\n",
    "    \\newcommand{\\sigman}{\\sigma_{\\epsilon}}\n",
    "    \\newcommand{\\degreeC}{^\\circ \\mathrm{C}}\n",
    "    \\newcommand{\\miss}{\\mathrm{miss}}\n",
    "    \\newcommand{\\obs}{\\mathrm{nearby}}\n",
    "    \\DeclareMathOperator*{\\softmax}{smoothmax}\n",
    "    \\DeclareMathOperator*{\\softmin}{smoothmin}\n",
    "    \\newcommand{\\error}{\\mathrm{err}}\n",
    "    \\newcommand{\\hour}{ {hr} }\n",
    "    \\DeclareMathOperator{\\kSESE}{k_{\\mathtt{SExSE}}}\n",
    "    \\DeclareMathOperator{\\kdiurn}{k_{\\mathtt{SESE\\_24}}}\n",
    "    \\DeclareMathOperator{\\ksumprod}{k_{\\mathtt{sumprod}}}\n",
    "    \\newcommand{\\iday}{\\mathtt{day}}\n",
    "    \\newcommand{\\dayset}[1]{\\cbr{i}_{#1}}\n",
    "    \\newcommand{\\discrepancy}{\\delta}\n",
    "    \\newcommand{\\Xmax}{X_{\\max}}\n",
    "    \\newcommand{\\Xmin}{X_{\\min}}\n",
    "    \\newcommand{\\Fcond}{F_{X \\mid \\Xmax,\\Xmin}}\n",
    "    \\newcommand{\\pxx}[2]{\\Pr{}_{#1#2}}\n",
    "    \\newcommand{\\pij}{\\pxx{i}{j}}\n",
    "    \\newcommand{\\pisum}{\\pxx{i}{\\bullet}}\n",
    "    \\newcommand{\\psumj}{\\pxx{\\bullet}{j}}\n",
    "    \\newcommand{\\eqlabel}[1]{\\label{#1}}\n",
    "    \\newcommand{\\thetagp}{\\theta_{\\GP}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2\n",
      "16.1\n"
     ]
    }
   ],
   "source": [
    "begin\n",
    "    hr = Hour(23)\n",
    "    low = Hour(8)\n",
    "    peak = Hour(20)\n",
    "    TnTx_low = waterloo_TnTx_dict[low]\n",
    "    TnTx_peak = waterloo_TnTx_dict[peak]\n",
    "    \n",
    "    apr3_low = TnTx_low[TnTx_low[:ts_day].values.==DateTime(2015,4,4),:]\n",
    "    apr3_peak = TnTx_peak[TnTx_peak[:ts_day].values.==DateTime(2015,4,4),:]\n",
    "    \n",
    "    global apr3_realmax = get(apr3_low[1,:Tx])\n",
    "    global apr3_measured = get(apr3_peak[1,:Tx])\n",
    "    println(apr3_realmax)\n",
    "    println(apr3_measured)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Long, high-quality records of temperature provide an important basis for our understanding of climate variability and change. Historically, there has been a focus on monthly-average temperature records, which are sufficient for certain analyses, such as quantifying long-term changes in temperature. As our knowledge of climate change expands, however, there is increasing interest in understanding changes in temperature on shorter timescales, with a particular focus on extreme events. To do so, it is necessary to utilize higher-resolution temperature data. \n",
    "\n",
    "Recent work has led to the development of the Global Historical Climatology Network-Daily (GHCND) database \\citep{menne2012overview}, which contains, among other variables, daily maximum and minimum temperatures from weather stations around the globe. The database draws from a range of different sources, and the data within it undergoes basic quality control to remove erroneous values. \n",
    "\n",
    "The current quality control methodology, however, does not account for so-called `inhomogeneities'. Inhomogeneities result from changes in measurement practices that impact the recorded temperatures. For temperature, known inhomogeneities include (a) changes in the time of observation, (b) changes in the thermometer technology, (c) station relocation, and (d) changes in land use around a station \\citep{menne2009us}. While these inhomogeneities have a small effect on, e.g., the estimation of global mean temperature, they can have a large effect on estimation of temperature variability and change at a more local scale.\n",
    "\n",
    "There is a large body of work focused on homogenizing monthly-average temperatures \\citep[e.g.,][]{karl1986model, easterling1996development, peterson1998homogeneity, ducre2003comparison, menne2009homogenization, vincent2012second}, resulting in widely available, large-scale homogenized monthly temperature datasets. Homogenization typically proceeds through identifying non-climatic `breakpoints' in a given time series through comparison with neighboring stations. Once a breakpoint is identified, the measurements recorded after the breakpoint are adjusted in some way to reduce or remove the inhomogeneity. Most applications of these methods, however, focus on adjusting the mean state of the data rather than the shape of the distribution \\citep[see][and references therein]{della2006method}. While this may be sufficient for monthly data, it is known that certain changes in measurement practices affect different percentiles of daily temperature in different ways. To address this issue, some homogenization methods have also employed percentile matching techniques, wherein the adjustment to a timeseries after a breakpoint is a function of percentile \\citep{della2006method, trewin2013daily}. \n",
    "\n",
    "Here, we focus primarily on addressing the time of observation bias, as well as its time trend, because of its known impact on the distribution of daily maximum and minimum temperature ($\\Tx$ and $\\Tn$) measurements. The bias exists because $\\Tx$ and $\\Tn$ are often recorded by an observer who visits a weather station every 24 hours, and notes the maximum and minimum temperatures measured by the thermometer over the previous 24 hours. Ideally, the observer would visit the station at midnight, and the highest and lowest temperatures over the past 24 hours would typically be representative of the high and low during the prior day. For convenience, however, most observers record data at a daytime hour instead. As can be seen in Fig.~XX, measurements recorded in the early morning may not properly register the low of the night before if it was unusually warm. Similarly, measurements recorded in the late afternoon may not properly register the high of the prior day if it was usually cool. In both cases, this will lead to a reduction in the variance of $\\Tx$ and $\\Tn$ distributions, but the effect will be greater at low (high) percentiles for $\\Tx$ ($\\Tn$). \n",
    "\n",
    "If the time of observation remained constant over time, the bias would still exist, but it would not be linked to spurious trends in the data. However, there have been known (and likely unknown) changes in the time of observation. In the United States, for example, observers were instructed to switch from recording data in the afternoon to recording data in the morning beginning in the 1950s. This change has led to an apparent decrease in both $\\Tx$ and $\\Tn$ over time \\citep{menne2009us}. \n",
    "\n",
    "The goal of our approach is to infer the true $\\Tx$ and $\\Tn$ values throughout the data records, thereby correcting both the variance biases and the spurious trends. This stands in constrast to previous work, which has focused primarily on addressing spurious trends. We approach the problem as a missing data problem, wherein we are trying to recover the values of $\\Tx$ and $\\Tn$ that may have been overwritten due to measurement practices. Furthermore, by employing a Gaussian process framework and nearby stations with hourly data, we are able to simulate multiple realizations of temperature timeseries at each station, thereby providing estimates of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of bias induces by measurement hour"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:illustrate_bias}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the measurement bias in daily maxima and minima with ten days of hourly temperature measurements from the Waterloo Municipal Airport station in Iowa. \n",
    "Ideally, $\\Tx$ measurements should capture the peak of each diurnal cycle, and $\\Tn$ its trough. \n",
    "In Figure \\ref{fig:waterloo_triangles}, those ideal measurements are indicated by the red and blue triangles respectively. \n",
    "The actual measurements are obtained by dividing the data into 24 hour measurement windows, and extracting the minimum and maximum. \n",
    "For each window, we plot these extrema with a red and blue horizontal line.\n",
    "\n",
    "![\n",
    "    \\label{fig:waterloo_triangles}\n",
    "    An extract of the temperature measurements made at the Waterloo Municipal Airport,\n",
    "    with emulated measurements of daily minima and maxima shown in blue and red respectively,\n",
    "    measured each day at 23:00 UTC.\n",
    "    The blue and red triangles indicate the coldest and warmest point of each diurnal cycle.\n",
    "    The occasional mismatches between the 24-hour extrema, and the peaks and troughs of the diurnal cycle induces a bias in the temperature record.\n",
    "    ](\n",
    "    ../figures/waterloo_triangles.png\n",
    "    )\n",
    "\n",
    "On most days, the ideal measurement and the actual measurement coincide: the triangle is on that day's line. \n",
    "But there are also several misses. The most blatant example occurs on April 3rd, \n",
    "where the peak of the diurnal cycle is 7.2°C and occurs at 21:00 UTC. \n",
    "However, because the previous day was much warmer, the day's $\\Tx$ record of 16.1°C is reached immediately after the previous day's measurement. \n",
    "The measured $\\Tx$ therefore overestimates the diurnal cycle's peak by 8.9°C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subtle bias in the daily records can in turn bias long-term summary statistics that are of climatological interest. \n",
    "A measure as simple as the average daily maximum temperature for an entire year (2015) increases by over 1°C if the measurements are made at the warmest time of day 21:00 UTC rather than 14:00 UTC (see Figure \\ref{fig:waterloo_avgTnTx}).\n",
    "Conversely, the average $\\Tn$ is colder by over 1°C if $\\Tn$ is measured at 10:00 UTC (the coldest time of day on average) rather than 17:00 UTC.\n",
    "\n",
    "![\n",
    "    \\label{fig:waterloo_avgTnTx}\n",
    "    Mean daily maximum temperature (left) \n",
    "    and mean daily minimum temperature (right) \n",
    "    for 2015 as a function of measurement time (UTC), \n",
    "    based on daily extrema emulated at four Iowa weather stations.\n",
    "    ](\n",
    "    ../figures/waterloo_avgTnTx.png\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A climatologist studying weather variability might be interested in summary statistics such as the average absolute change in the daily temperature maxima and minima from one day to the next.\n",
    "The answer to that question too depends on the time of day at which the temperatures are recorded.\n",
    "Collecting the measurements at the hottest time of day means that the peaks on a warm day gets recorded twice, \n",
    "erasing the diurnal peaks of the following colder day, \n",
    "and hence the variability gets underestimated.\n",
    "We can see this in Figure \\ref{fig:waterloo_meanabsdiff}), where the respective variability estimates drop if the maxima get measured at the warmest time, or if the minima get measured at the coldest time.\n",
    "\n",
    "![\n",
    "\\label{fig:waterloo_meanabsdiff}\n",
    "    Mean absolute daily change in maximum temperature (left) \n",
    "    and mean absolute daily change in minimum temperature (right) \n",
    "    for 2015 as a function of measurement time (UTC), \n",
    "    based on daily extrema emulated at four Iowa weather stations.\n",
    "](../figures/waterloo_meanabsdiff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed solution\n",
    "\n",
    "We have seen that the daily maxima and minima do not faithfully record each diurnal cycle's peak and trough. \n",
    "The peaks on a relatively cold day can get overwritten by temperatures at either end of the measurement window that properly belong to the previous or the next diurnal cycle. \n",
    "Troughs on relatively warm days can be similarly overwritten. \n",
    "Our goal is to undo this damage, and recover estimates of summary statistics, such as the average daily maximum temperature, that do not suffer from the consequent bias. \n",
    "We need to address the erasure of information caused by the measurement mechanism, and therefore view this as a missing data problem.\n",
    "\n",
    "Taking the missing data perspective, we seek to impute the hourly temperatures that have been replaced by a maximum and minimum over a 24 hour period. \n",
    "To do so, we combine information from two sources: the recorded daily temperature extremes at the station of interest, and also hourly temperatures recorded at nearby meteorological stations. \n",
    "These hourly measurements are considered less reliable by climatologists, as they aren't as carefully documented, calibrated, and situated. \n",
    "The meterological stations are often in locations (like airports) where human activity will affect temperatures.\n",
    "Therefore, summary statistics extracted directly from those measurements would not be directly usable for climatology, as they could suffer from systematic bias.\n",
    "However, even if miscalibrated, the meterological data do contain valuable information about the hourly changes in temperatures on any given day.\n",
    "We therefore use them to inform the shape of the imputed temperature time-series at our location of interest,\n",
    "while we use the recorded temperature extrema to calibrate and constrain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Spatiotemporal Model\n",
    "\n",
    "To model measured temperatures at various locations and times, we use a spatio-temporal Gaussian process model. \n",
    "In its simplest form, we believe that temperatures from stations that are near each other are more correlated than distant stations, and that those correlations should also decay in time. \n",
    "In the spatial statistics literature, squared exponential covariance functions are commonly used to model correlations decaying as a function of distance. \n",
    "Ignoring the time dimension, we would model the simultaneous temperatures throughout a region as a Gaussian process, with the covariance of two locations $\\xvec$ and $\\xvec'$\n",
    "\\begin{equation}\n",
    "    \\label{eq:kspace}\n",
    "    \\cov\\del{T(\\xvec), T(\\xvec') \\mid t} = k_{space}(\\xvec, \\xvec') = \\sigmaf^2 \\exp\\del{-\\frac{\\del{\\xvec-\\xvec'}\\trans\\del{\\xvec-\\xvec'}}{2\\ell_x^2}}\\,.\n",
    "\\end{equation}\n",
    "Similarly, ignoring the spatial dimension, the time series of temperatures at a single location can be modeled as a Gaussian process with covariance between two moments $t$ and $t'$\n",
    "\\begin{equation}\n",
    "\\cov\\del{T(t), T(t') \\mid \\xvec} = k_{time}(t, t') = \\sigmaf^2 \\exp\\del{-\\frac{\\del{t-t'}^2}{2\\ell_t^2}}\\,.\n",
    "\\end{equation}\n",
    "We then combine the spatial and temporal model by multiplying the covariance functions\n",
    "\\begin{equation}\n",
    "k_{st}(\\xvec,\\xvec',t,t') = k_{time}(t,t') \\cdot k_{space}(\\xvec, \\xvec')\\,.\n",
    "\\end{equation}\n",
    "This gives us the covariance of the Gaussian process underlying the full spatio-temporal model of temperatures.\n",
    "To complete the model specification, we add a mean temperature for each station $\\mu_{\\station{i}}$, and iid measurement noise $\\epsilon_i$.\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\T_i &= \\mu_{\\station{i}} + f(\\xvec_i, t_i) + \\epsilon_i\\\\\n",
    "    f(\\xvec_i, t_i) &\\sim \\GP\\del{0, k_{st}(\\xvec,\\xvec',t,t')}\\\\\n",
    "    \\epsilon_i &\\overset{\\iid}{\\sim} \\normal\\del{0,\\sigman^2}\\\\\n",
    "\\end{split}\n",
    "\\eqlabel{eq:gpmodel}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the spatiotemporal model\n",
    "\n",
    "Software is readily available in many programming languages for fitting Gaussian process models, including inference on the covariance parameters. We chose to use the julia `GaussianProcesses.jl` package to fit the above spatiotemporal model to the hourly temperatures at four Iowa weather stations. However, the Iowa data includes 47,864 measurements, which is computationally challenging to fit directly with a single Gaussian process. \n",
    "While approximation techniques exist to fit such large datasets, we chose the less efficient but simpler approach of dividing the data into 10-day chunks, modeled as independent Gaussian processes with shared hyperparameters.\n",
    "We put vague normal priors on $\\mu_{\\station{i}}$ with large variance $\\sigma_{\\mu}^2$, which can be incorporated into the Gaussian process with an additional term\n",
    "\\begin{equation}\n",
    "    k_{\\mu}(\\xvec, \\xvec') = \\sigma_\\mu^2 \\delta\\cbr{\\xvec-\\xvec'}\n",
    "\\end{equation}\n",
    "added to the covariance function so that the spatio-temporal kernel becomes\n",
    "\\begin{equation}\n",
    "    k_{st}(\\xvec,\\xvec',t,t') = k_{time}(t,t') \\cdot k_{space}(\\xvec, \\xvec') + k_\\mu(\\xvec, \\xvec') \\,.\n",
    "\\end{equation}\n",
    "The model therefore has 4 free parameters: $\\sigmaf$, $\\ell_t$, $\\ell_x$ and $\\sigman$. \n",
    "We optimize the marginal likelihood of the Iowa data as a function of these three parameters\n",
    "\\begin{equation}\n",
    "\\label{eq:optimization}\n",
    "\\hat\\sigmaf,\\hat\\ell_t,\\hat\\ell_x,\\hat\\sigman = \\argmax_{\\sigmaf,\\ell_t,\\ell_x,\\sigman} \\cbr{ \\Pr\\del{ Y \\mid {\\sigmaf,\\ell_t,\\ell_x,\\sigman} } }\\,,\n",
    "\\end{equation}\n",
    "and obtain $\\hat\\sigmaf=3.73\\,\\degreeC$, $\\hat\\ell_t=2.7\\,\\mathrm{hours}$, $\\hat\\ell_x=176.4\\,\\mathrm{km}$ and $\\hat\\sigman=0.44\\,\\degreeC$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions using nearby data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:predict_nearby}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:imputations_2x2}Imputations of the temperature time series at Waterloo Municipal Airport between May 28, 2015 and June 1, 2015 (a) using only nearby data and the product of squared exponentials model; (b) using only nearby data and the sum of products model; (c) incorporating $\\Tn$ and $\\Tx$ measurements under the product of squared exponentials model; and (d) incorporating $\\Tn$ and $\\Tx$ measurements under the sum of products model. The mean is subtracted from each time series in (a) and (b) as the models leave the average temperature at the imputation site as a free parameter. For each imputation distribution, the mean is shown as a thick line, surrounded by an 80% credible envelope in lighter color, and example imputations as thinner lines.](../figures/imputations_2x2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a spatio-temporal Gaussian process model with optimized covariance parameters, we can use it to generate predictions at the station where we aim to generate imputations based on nearby measurements.\n",
    "Gaussian processes make this a closed-form procedure. We'll denote the temperatures we wish to impute as $\\T_\\miss{}$ at times $t_\\miss$ and location $\\xvec_\\miss$ and those observed at nearby stations as $\\T_\\obs{}$, at times $t_\\obs$ and locations $X_\\obs$. \n",
    "Under the spatio-temporal model, $\\T_\\miss$ and $\\T_\\obs$ are jointly multivariate normal, with mean zero and covariance given by $k_{st}(\\xvec,\\xvec',t,t')$. \n",
    "Standard results for conditioning within multivariate normals then yields\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\T_\\miss \\mid \\T_\\obs &\\sim \\normal\\del{\\mu_{\\miss \\mid \\obs}, \\Sigma_{\\miss \\mid \\obs}}\\,, \\\\\n",
    "    \\mu_{\\miss \\mid \\obs} &= \\E \\del{\\T_\\miss \\mid \\T_\\obs} \\\\\n",
    "        &= \\cov\\del{\\T_\\miss, \\T_\\obs} \\cov\\del{\\T_\\obs, \\T_\\obs}^{-1} \\T_\\obs\\,, \\\\\n",
    "    \\Sigma_{\\miss \\mid \\obs} &= \\var \\del{\\T_\\miss \\mid \\T_\\obs} \\\\\n",
    "        &= \\cov\\del{\\T_\\miss,\\T_\\miss} - \\cov\\del{\\T_\\miss, \\T_\\obs} \\cov\\del{\\T_\\obs, \\T_\\obs}^{-1} \\cov\\del{\\T_\\obs, \\T_\\miss}\\,. \\\\ %_\n",
    "\\end{split}\n",
    "\\eqlabel{eq:unconstrained_post}\n",
    "\\end{equation}\n",
    "All covariance matrices can be obtained by plugging into $k_{st}$. For example, the $ij$th entry of $\\cov\\del{\\T_\\miss, \\T_\\obs}$ is given by $k_{st}(\\xvec_\\miss,X_\\obs\\sbr{j},t_\\miss\\sbr{i},t_\\obs\\sbr{j})$, where $X_\\obs\\sbr{j}$ gives the spatial covariates of the $j$th observation, and $t_\\obs\\sbr{j}$ its time.\n",
    "\n",
    "In Figure \\ref{fig:imputations_2x2}(a), we show an example of predictions obtained from this spatio-temporal model. We witheld measurements from the Waterloo Municipal Airport, and then used data from three nearby stations between May 2, 2015 and May 5, 2015 to predict the Waterloo temperatures during the same time window. This allows us to assess the quality of the predictions on this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputations\n",
    "\n",
    "## Imputing by Conditioning on Extrema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim isn't simply to predict temperatures at a location with no measurements, but rather to impute hourly temperatures at a location with accurate measurements of the daily temperature extrema.\n",
    "This is an instance of a more general statistical problem: if a random $p$-vector $\\cbr{X_i:~i=1,\\ldots,p}$ has a known distribution $F_X$, and its maximum $\\Xmax \\equiv \\max_i\\cbr{X_i}$ and minimum $\\Xmin \\equiv \\min_i\\cbr{X_i}$ are measured, how does one draw samples from $\\Fcond$, the distribution of $X$ conditional on $\\Xmax$ and $\\Xmin$?\n",
    "Conditional draws from $\\Fcond$ need to respect three constraints: one component of $X$ must be equal to $\\Xmin$, another to $\\Xmax$, and all other components must lie between $\\Xmin$ and $\\Xmax$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, we could implement a valid imputation algorithm by drawing random samples $F_X$,\n",
    "and only keeping the samples that satisfy the three constraints. \n",
    "Unfortunately, if $F_X$ is a continuous distribution, the probability of a random draw from $F_X$ satisfying such sharp constraints is zero.\n",
    "One could envision adding some tolerance, so that samples with minimum and maximum within $\\epsilon$ of $\\Xmax$ and $\\Xmin$ are retained, but as the dimensionality $p$ grows, the rejection probability will rapidly go to 1, thus requiring huge sample sizes.\n",
    "Ultimately, this rejection sampling strategy is therefore bound to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chain Monte Carlo (MCMC) techniques can also be used to draw samples from arbitrary distributions with densities known up to a constant. The density of $\\Fcond$ is obtained up to a constant multiplier through a simple application of Bayes' theorem. It is proportional to the prior density of $F_X$ multiplied by indicators ensuring that the extrema are respected.\n",
    "\\begin{equation}\\begin{split}\n",
    "    \\Pr\\del{X \\mid \\Xmax,\\Xmin} &\\propto \\Pr\\del{X} \\Pr\\del{\\Xmax,\\Xmin \\mid X} \\,, \\\\\n",
    "           &\\propto \\Pr\\del{X} \\Ind\\del{ \\max_i\\cbr{X_i} = \\Xmax }\\Ind\\del{ \\min_i\\cbr{X_i} = \\Xmin } \\,.\n",
    "\\end{split}\n",
    "\\eqlabel{eq:bayes_exact}\n",
    "\\end{equation}\n",
    "However, once again, this distribution is zero everywhere in $\\mathbb{R}^p$, except in a (p-2) dimensional subspace where the $\\min$ and $\\max$ constraints are met. \n",
    "This doomed the rejection sampler, and will also prevent any unmodifed MCMC algorithm from converging onto $\\Fcond$.\n",
    "We therefore approximate the constraint by replacing the likelihood term $\\Pr\\del{\\Xmax,\\Xmin \\mid X}$ with two narrow independent normal distributions around the minimum and maximum of $X$. \n",
    "This “softens” the conditional distribution,\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\Pr\\del{X \\mid \\Xmax,\\Xmin} &\\propto \\Pr\\del{X} \n",
    "                                         \\normal\\del{\\Xmax \\mid \\max_i\\cbr{X_i}, \\epsilon^2}\n",
    "                                         \\normal\\del{\\Xmin \\mid \\min_i\\cbr{X_i}, \\epsilon^2}\\,,\n",
    "\\end{split}\n",
    "\\eqlabel{eq:normal_lik}\n",
    "\\end{equation}\n",
    "where $\\normal\\del{x \\mid \\mu, \\sigma^2}$ is the density of a normal distribution with mean $\\mu$ and variance $\\sigma^2$ evaluated at $x$. \n",
    "For small $\\epsilon$, this is a very tolerable approximation which enables the use of MCMC techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:constraints3d}\n",
    "With three variables $X_1$, and $X_2$ and $X_3$, $\\Fcond$ resides in the one-dimensional six-sided loop shown with thicker green lines. This is a 1D manifold embedded in 3D space, and possessing sharp corners, making it difficult for most MCMC algorithms to explore.\n",
    "](../figures/constraints3d.png)\n",
    "\n",
    "This approximation to $\\Fcond$ remains a difficult distribution to sample from. \n",
    "We illustrate the constraint in a 3-dimensional setting in Figure \\ref{fig:constraints3d}.\n",
    "The MCMC algorithm must travel efficiently along the six edges of the allowed subspace,\n",
    "and navigate corners when the index of the extremum components change.\n",
    "Hamiltonian Monte Carlo (HMC) has shown a a remarkable ability to navigate complicated distributions, including distributions where the typical set has “pinch points” of strong\n",
    "curvature <cite data-cite=\"betancourt2017conceptual\">(Betancourt, 2017)</cite>, similar to the “corners“ in $\\Fcond$. \n",
    "We therefore used HMC as implemented by the Stan probabilistic programming language <cite data-cite=\"stancite\">(Carpenter, 2017)</cite> to obtain draws from $\\Fcond$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMC's efficient sampling relies on gradient information in order to move towards regions of high probability.\n",
    "The normal likelihood \\eqref{eq:normal_lik} softened the extrema constraints,\n",
    "but the maximum and minimum functions also remove information from the gradient.\n",
    "The partial derivative of the log-likelihood of the maximum term with respect to $X_i$ is proportional to\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\log \\normal\\del{\\Xmax \\mid \\max_i\\cbr{X_i}, \\epsilon^2}}{\\partial X_i} \\propto \\del{\\Xmax - X_i} \\Ind\\cbr{\\argmax_j\\del{X_j} = i} \\,,\n",
    "\\end{equation}\n",
    "where $\\argmax$ is the function that returns the index of the maximum component.\n",
    "In other words, the gradient pulls the maximum of the current sample towards $\\Xmax$,\n",
    "and ignores all other components.\n",
    "This makes it difficult for HMC to efficiently explore scenarios where other components are the maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assist the HMC algorithm with another approximation.\n",
    "We replace the $\\max$ and $\\min$ functions with the $\\softmax$ and $\\softmin$ functions, which take real inputs $x_1, \\ldots, x_p$ and a sharpness parameter $k$ and return\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\softmax\\del{x_1, \\ldots, x_p ; k} &= \\frac{1}{k} \\log\\del{\\sum_{i=1}^p e^{kx_i}} \\\\\n",
    "    \\softmin\\del{x_1, \\ldots, x_p ; k} &= -\\softmax\\del{-x_1, \\ldots, -x_p; k}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "As $k \\rightarrow \\infty$, $\\softmax$ becomes the maximum, and $\\softmin$ becomes the minimum. When $\\softmax$ replaces $\\max$ and $\\softmin$ replaces $\\min$, there is a small price in precision due to the approximation, but there is a huge computational benefit: the gradient is now informative for all components of $X$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\log \\normal\\del{\\Xmax \\mid \\softmax\\del{X_{1:p} ; k}, \\epsilon^2}}{\\partial X_i} \\propto \\del{\\Xmax - \\softmax\\del{X_{1:p} ; k}} \n",
    "        \\frac{e^{k X_i}}\n",
    "             {\\sum_{j=1}^p e^{k X_j}} \\,.\n",
    "\\end{equation}\n",
    "These modifications make HMC a viable algorithm to efficiently draw samples from the constrained posterior. \n",
    "Setting $k$ and $\\sigma_\\epsilon$ is a compromise between exactness and efficiency; \n",
    "we found $k=10$ and $\\sigma_\\epsilon=0.1$ to perform well for this paper's application.\n",
    "\n",
    "In the rest of this paper, we will refer to this use of HMC and a smoothmax approximation to the target distribution as SmoothHMC.\n",
    "SmoothHMC provides a generally applicable algorithm to draw from a multivariate distribution conditionally on the observed minimum and maximum of its components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of Hamiltonian Monte Carlo with Smoothmax Approximation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:toy_example}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify SmoothHMC's ability to obtain draws from $\\Fcond$ in a simplified setting where the distribution function of $\\Fcond$ can be derived analytically and also computed easily.\n",
    "In our application, $F_X$ is the posterior predictive multivariate normal distribution $\\T_\\miss \\mid \\T_\\obs$ obtained from nearby measurements, with mean and marginal variance evolving smoothly from one prediction to the next.\n",
    "To retain a resemblence to this, we specify a random vector $X$ with each component $X_i$ normally distributed, and with sinusoidal means and variances, but without any correlations between them,\n",
    "so as to avoid a combinatorial explosion when obtaining the distribution function analytically:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X_i &\\sim \\normal \\del{\\mu_i, \\sigma_i} \\,, \\\\\n",
    "X_i & \\indep X_j ~\\forall\\, i \\neq j \\,, \\\\\n",
    "\\Xmax &= \\max_i\\cbr{X_i} \\,, \\\\\n",
    "\\Xmin &= \\min_i\\cbr{X_i} \\,,\\\\\n",
    "\\mu_i &= 10 + \\sin\\del{2\\pi i / 50} \\,, \\\\\n",
    "\\sigma_i &= 0.1+\\cos^2\\del{2\\pi i / 50} \\,, \\\\\n",
    "i &= 1, 2, \\ldots, 100 \\,.\n",
    "\\end{split}\n",
    "\\eqlabel{eq:toyspec}\n",
    "\\end{equation}\n",
    "The unconstrained distribution of $X_i$ is illustrated in Figure \\ref{fig:toy_quantiles}(a).\n",
    "In this example, we aim to sample from the distribution of $X_i$ subject to the observation that $\\Xmax=12.5$ and $\\Xmin=8.8$.\n",
    "We chose this example to have an analytically and computationally constrained distribution $\\Fcond$ (see derivation in Appendix \\ref{sec:analytical_posterior}) so that we can verify the correctness of the imputations.\n",
    "The marginal quantiles of the analytical posterior are shown in Figure \\ref{fig:toy_quantiles}(b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:toy_quantiles}(a) Prior distribution of $X_i$ displayed as mean function with $2\\sigma$ envelope; (b) Quantiles of the analytically derived posterior $\\Fcond$ conditioned on $\\Xmin$ and $\\Xmax$, with prior $\\mu_i$ shown in blue; (c) Quantiles of the samples drawn from $\\Fcond$ using Stan without the $\\softmax$ approximation, with prior $\\mu_i$ shown in blue; (d) Quantiles of the samples drawn from $\\Fcond$ using Stan with the $\\softmax$ approximation, with prior $\\mu_i$ shown in blue.](../figures/toy_quantiles.png)\n",
    "\n",
    "To obtain samples from $\\Fcond$, we use the implementation of Hamiltonian Monte Carlo provided by the probabilistic programming language Stan.\n",
    "In Stan, the user specifies a probabilistic data-generating process for the observed data, based on parameters and latent variables with accompanying priors.\n",
    "Stan then compiles this model into a custom `C++` program that implements\n",
    "posterior sampling using HMC.\n",
    "We implement two Stan models to draw from $\\Fcond$.\n",
    "The Stan model code for both are available in appendix \\ref{sec:stan_illustration}.\n",
    "The first model implements equation \\eqref{eq:normal_lik}, \n",
    "with the narrow normal likelihood term around the maximum and minimum, \n",
    "while the second model includes the $\\softmax$ and $\\softmin$ approximations to the maximum and minimum functions.\n",
    "For each Stan model, we obtain 4 HMC chains each with 10,000 warm-up samples followed by 10,000 samples.\n",
    "The quantiles of the samples obtained without the $\\softmax$ approximation are shown in Figure \\ref{fig:toy_quantiles}(c).\n",
    "By default, Stan initizializes each $X_i$ uniformly at random between -2 and 2,\n",
    "and for most variables, the algorithm remains stuck near the initial values.\n",
    "Furthermore, most samples do not conform to the constraints imposed by the observed $\\Xmin$ and $\\Xmax$ values, which further invalidates these results.\n",
    "However, once we replace the maximum function with the $\\softmax$ function,\n",
    "with quantiles shown in Figure \\ref{fig:toy_quantiles}(d),\n",
    "Stan is able to draw samples that respect the observed extrema.\n",
    "Furthermore, a visual comparison of the analytical quantiles in Figure \\ref{fig:toy_quantiles}(a)\n",
    "and the Stan sample quantiles in Figure \\ref{fig:toy_quantiles}(d) confirms that \n",
    "this sampling algorithm delivers a close approximation of the marginal distribution of each variable $X_i$ in $\\Fcond$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:toy_joint}\n",
    "Comparison of the joint distribution of $X_{23}$ and $X_{52}$\n",
    "obtained analytically and from Stan samples.\n",
    "](../figures/toy_joint.png)\n",
    "\n",
    "We may also wish to verify that Stan samples correctly from the *joint* distribution of any combination of variables. \n",
    "We do this visually for a pair of variables, $X_{23}$ and $X_{52}$, with results shown in Figure \\ref{fig:toy_joint}.\n",
    "In that figure, the central scatterplot shows the 40,000 Stan samples obtained using the $\\softmax$ approximation.\n",
    "Superimposed on the scatterplot are a contour plot (with dash-dotted lines) of the probability distribution function of the analytical conditional distribution $\\Fcond$ of $X_{23}$ and $X_{52}$ when neither $X_{23}$ nor $X_{52}$ is one of the extrema, multiplied by the probability of that being the case, which is available analytically.\n",
    "This can be compared to the contour plot (solid lines) of the same probability distribution function obtained through a kernel density estimator of the subset of Stan samples where neither $X_{23}$ and $X_{52}$ is the minimum or maximum,\n",
    "using a normal kernel with bandwidth $0.2$,\n",
    "and multiplied by the proportion of samples where that is the case.\n",
    "The kernel estimates are divided by the integrated probability mass of the kernel that is inside of the boundaries, in order to reduce boundary effects.\n",
    "The thin black dotted line are one kernel bandwidth away from the $\\Xmin$/$\\Xmax$ boundaries.\n",
    "Outside the thin black dotted line, the kernel density estimates are less trustworthy.\n",
    "The four histograms around the scatter plot are of the Stan samples where one of the variables is an extremum, weighted so as to integrate to the fraction of samples that satisfy that condition.\n",
    "For example, the top histogram is of $X_{23}$ for samples where $X_{52}$ is the maximum,\n",
    "and integrates to the fraction of samples where that is the case.\n",
    "The super-imposed pink line is that of a truncated normal probability distribution function\n",
    "multiplied by the probability of the satisfied condition.\n",
    "For example, the pink line over the top histogram integrates to $\\pxx{\\bullet}{52}$.\n",
    "Lastly, the blue and red lines are $\\Xmin$ and $\\Xmax$ respectively.\n",
    "There is a close match between the contours of the analytical joint distribution function (dash-dotted lines) and of the kernel density estimate of the Stan samples.\n",
    "Each of the four histogram of samples where $X_{23}$ or $X_{52}$ occupies the minimum or maximum position matches the corresponding analytical distribution function.\n",
    "This visual comparison of the sample and analytical distributions should reassure us that Stan is yielding a good approximation of a sample drawn from the true $\\Fcond$ in this example.\n",
    "We did not examine the behavior of the sampling algorithm for the joint distribution of more than two variables due to the difficulty of visualizing such a distribution,\n",
    "but we see no reason to suspect that the algorithm suffers from pathological behavior that does not manifest itself in these univariate and bivariate inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothmax Temperature Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with the SmoothHMC algorithm implemented in Stan, we finally return to the problem of imputing hourly temperature measurements.\n",
    "To impute the missing temperatures, we need to draw from the posterior distribution $\\T_\\miss \\mid \\T_\\obs, \\Tn, \\Tx$.\n",
    "Bayes' theorem conditional on $\\T_\\obs$ gives\n",
    "\\begin{equation}\n",
    "    \\Pr\\del{\\T_\\miss \\mid \\T_\\obs, \\Tn, \\Tx} = \\frac{\n",
    "        \\Pr\\del{\\Tn, \\Tx \\mid \\T_\\obs, \\T_\\miss } \n",
    "        \\Pr\\del{\\T_\\miss \\mid \\T_\\obs}\n",
    "        }{\n",
    "        \\Pr\\del{\\Tn, \\Tx \\mid \\T_\\obs}\n",
    "        }\\,.\n",
    "\\end{equation}\n",
    "The second term in the numerator is the posterior obtained in section \\ref{sec:predict_nearby} now acting as a prior.\n",
    "The denominator is a normalizing constant.\n",
    "The first term in the numerator is either zero or one, indicating whether $\\T_\\miss$ satisfies the constraint imposed by the observed $\\Tn$ and $\\Tx$.\n",
    "Therefore, the posterior distribution takes a similar form to \\eqref{eq:bayes_exact}, which motivates the use of SmoothHMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small leap of faith is needed to accept that SmoothHMC's success in a toy example in section \\ref{sec:toy_example} will extend to this application.\n",
    "There a three important differences between the toy example and the temperature time series model.\n",
    "Firstly, $\\Fcond$ is now a multivariate normal distribution with strong correlations obtained as the posterior distribution of a Gaussian process in section \\ref{sec:predict_nearby}.\n",
    "Secondly, instead of a single minimum and maximum, we observe extrema for every 24 hour period.\n",
    "Thirdly, we allow for the mean temperature to be different at different locations,\n",
    "and so the imputed temperatures are shifted by an additional parameter $\\mu_{\\miss}$,\n",
    "to which we will attach a vague prior.\n",
    "To summarize, the probabilistic model that we wish to draw posterior imputations of $\\T_\\miss$ from is given in \\eqref{eq:idealmodel}.\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    \\mu_{\\miss} &\\sim \\normal\\del{0,100} & \\text{ (vague prior on mean temperature)} \\\\\n",
    "    f_\\miss &\\sim \\normal\\del{\\mu_{\\miss \\mid \\obs}, \\Sigma_{\\miss \\mid \\obs}} & \\text{ (posterior from $\\T_\\obs$ becomes prior)} \\\\\n",
    "    \\T_\\miss &= \\mu_\\miss + f_\\miss \\\\\n",
    "    \\Tx\\sbr{\\iday} &= \\max_{i \\in \\dayset{\\iday}}\\cbr{ \\T_{\\miss,i}} & \\text{ (observe maximum in 24hr window)}\\\\\n",
    "    \\Tn\\sbr{\\iday} &= \\min_{i \\in \\dayset{\\iday}}\\cbr{ \\T_{\\miss,i}} & \\text{ (observe minimum in 24hr window)}\\\\\n",
    "    \\dayset{\\iday} &= \\cbr{i : \\iday-1+\\frac{\\hour}{24} \\lt t_{\\miss,i} \\le \\iday + \\frac{\\hour}{24}} & \\text{ (indices of times in the 24hr window)}\n",
    "\\end{aligned}\n",
    "\\eqlabel{eq:idealmodel}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from this model, we modify it with the $\\softmax$ approximation to the maximum, and a normal likelihood. The resulting model is shown in \\eqref{eq:smoothed_model}, and the corresponding Stan code is in Appendix \\ref{sec:appendix_stan}.\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\mu_\\miss &\\sim \\normal\\del{0,100} \\\\\n",
    "    f_\\miss &\\sim \\normal\\del{\\mu_{\\miss \\mid \\obs}, \\Sigma_{\\miss \\mid \\obs}} \\\\\n",
    "    T_\\miss &= \\mu_\\miss + f_\\miss \\\\\n",
    "    \\Tx\\sbr{\\iday} &\\sim \\normal\\del{\\softmax_{i \\in \\dayset{\\iday}} \\cbr{ T_{\\miss,i}; k=10}, 0.1^2} \\\\\n",
    "    \\Tn\\sbr{\\iday} &\\sim \\normal\\del{\\softmin_{i \\in \\dayset{\\iday}} \\cbr{ T_{\\miss,i}; k=10}, 0.1^2}\n",
    "\\end{split}\n",
    "\\eqlabel{eq:smoothed_model}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example imputations from this procedure are shown in Figure \\ref{fig:imputations_2x2}(c). \n",
    "From May 28, 2015 to June 1, 2015, hourly temperatures are imputed at Waterloo Airport, using the hourly temperature measurements from nearby stations to inform the course of the temperatures, and using the daily minima and maxima “measurements” to constrain the imputed temperatures.\n",
    "One can verify visually that the imputations respect the $\\Tn$ and $\\Tx$ constraints, reaching but not exceeding each extreme on each day.\n",
    "Because we actually have hourly data for Waterloo, yet only fed our algorithm a reduction of this data to daily extremes, we can also plot the hidden temperatures (in black), and see how faithfully the imputations reproduce them.\n",
    "We see that the imputations indeed track the true measurements very closely. \n",
    "On May 31st, we can see that the imputations capture two possibilities: the $\\Tx$ record *could* have been set early in the 24-hr period, or at its end.\n",
    "This success demonstrates that the “leap of faith” was justified, and that SmoothHMC is capable of imputing temperature time series from the posterior distribution $\\T_\\miss \\mid \\T_\\obs, \\Tn, \\Tx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics\n",
    "\n",
    "## Variogram\n",
    "\n",
    "We can visually inspect our model by plotting temporal and spatial semi-variograms. The semi-variogram of a stationary spatio-temporal function $Y(\\xvec,t)$ is a function of the spatial lag $\\hvec$ and the temporal lag $r$\n",
    "\\begin{equation}\n",
    "    \\gamma\\del{\\hvec,r} = \\frac{1}{2} \\E\\sbr{\\del{Y\\del{\\xvec,t}-Y\\del{\\xvec+\\hvec,t+r}}^2} = \\var\\del{ Y\\del{\\xvec,t}} - \\cov\\del{ \\del{Y\\del{\\xvec,t}} , Y\\del{\\xvec+\\hvec,t+r}}\\,.\n",
    "\\end{equation}\n",
    "For a Gaussian Process model, with a stationary kernel $k(\\hvec,r)=k(\\xvec,\\xvec+\\hvec,t,t+r)$ this can be expressed in terms of the observation noise $\\sigman$ and kernel function $k(\\cdot,\\cdot)$, as\n",
    "\\begin{equation}\n",
    "    \\label{eq:gp_variogram}\n",
    "    \\gamma\\del{\\hvec,r} = \\sigman^2 + k\\del{0,0} - k(\\hvec,r)\\,.\n",
    "\\end{equation}\n",
    "From the data, the semi-variogram can also be estimated empirically, be averaging the square differences of any two observations that are separated by $\\hvec$ in space, and $r$ in time (or, in practice, within half a bin width of $\\hvec$ and $r$). By comparing the empirical variogram to the variogram of our fitted $\\GP$ model, we obtain a visual diagnosis of the model.\n",
    "\n",
    "In our Iowa example, there are only four possible locations. For each location, we plot the empirical temporal variogram $\\hat\\gamma\\del{0,r}$. For any pair of stations separated by $\\hvec$ (fixed), we can also plot $\\hat\\gamma\\del{\\hvec,r}$. We then overlay the model's semi-variogram obtained through equation \\eqref{eq:gp_variogram}, resulting in Figure \\ref{fig:spatial_variogram}.\n",
    "\n",
    "![\\label{fig:spatial_variogram} Semi-variogram](../figures/spatial_variogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the variogram of the simple `SExSE` model tracks the empirical variogram well at short lags, but fails to capture the diurnal cycle, and the fit degrades at long lag. We attempt to improve the model in section \\ref{sec:improving_model}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and expected error\n",
    "\n",
    "The variogram gives us a visual diagnostic of the overall model fit. To quantify the model's predictive ability in the Iowa example, we compare the posterior mean temperature to the withheld truth, and obtain the empirical mean squared error as\n",
    "\\begin{equation}\n",
    "    \\label{eq:mse}\n",
    "    \\mse\\del{\\error \\mid \\T_\\obs,\\Tx,\\Tn} = \\frac{1}{N} \\sum_{i=1}^N \\sbr{\\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs,\\Tx,\\Tn} - \\T_{\\miss,i}}^2\\,.\n",
    "\\end{equation}\n",
    "This equation is for the final predictions obtained using nearby hourly temperatures and local daily maxima and minima. \n",
    "A similar diagnostic can be computed for the intermediary predictions, which exclude the local $\\Tx$ and $\\Tn$ information. \n",
    "At that stage, we are not concerned with any overall bias in the predicted temperatures, so we instead compute the sample variance of the errors as\n",
    "\\begin{equation}\n",
    "    \\eqlabel{eq:varerr}\n",
    "    \\var\\del{\\error \\mid \\T_\\obs} = \\var_i \\cbr{\\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs} - \\T_{\\miss,i}}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model       | Log Likelihood | Var(err) | $\\E$(Var(err)) | MSE(err) | $\\E$(MSE(err)) |\n",
    "|-------------|----------------|----------|----------------|----------|----------------|\n",
    "| $\\kSESE$    | -55,614        | 1.589    | 0.682          | 1.115    | 0.44239        |\n",
    "| $\\kdiurn$   | -54,472        | 1.633    | 0.782          | 1.151    | 0.521          |\n",
    "| $\\ksumprod$ | -45,944        | 1.319    | 1.193          | 1.037    | 0.812          |\n",
    "Table: Model diagnostics for three Gaussian process covariance functions. \\label{table:diagnostics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, it isn't sufficient for the spatio-temporal model to yield good predictions; we also require a good estimate of its own accuracy. \n",
    "We estimate the expected MSE and predictive variance by sampling $K$ random draws $\\T^k_\\miss$ from the posterior distribution, again conditioned firstly on just $\\T_\\obs$ after fitting the spatio-temporal Gaussian process model, and then additionally on $\\T_\\obs$, $\\Tx$ and $\\Tn$ after incorporating the local data using Stan.\n",
    "The draws are obtained from the posterior multivariate normal distribution in the first case, and the MCMC samples obtained through Stan in the second case.\n",
    "We then evaluate the variance or MSE between the samples and the posterior mean as\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\E \\del{\\var\\del{\\error \\mid \\T_\\obs}} &\\approx \\frac{1}{K} \\sum_{k=1}^K \\var_i \\cbr{\\T^{(k)}_{\\miss,i} - \\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs}} \\\\\n",
    "    \\E \\del{\\mse\\del{\\error \\mid \\T_\\obs,\\Tx,\\Tn}} &\\approx \\frac{1}{K} \\sum_{k=1}^K \\mse_i \\cbr{\\T^{(k)}_{\\miss,i} - \\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs,\\Tx,\\Tn}} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "When evaluating models, we want the errors to be small, and so the empirical error variance and MSE to be low. A well-calibrated model should also have the expected error variances $\\E \\del{\\var\\del{\\error \\mid \\cdot}}$ close to their empirical values.\n",
    "\n",
    "These diagnostics for our first spatio-temporal model, the product of squared exponentials, are found in the first row of Table \\ref{table:diagnostics}. The empirical error variance using only nearby measurements is already fairly low, with typical errors of order $\\sqrt{1.589}=1.26\\,\\degreeC$. Incorporating the local measurements reduces it further to $\\sqrt{1.115}=1.06\\,\\degreeC$. However, the model is overly optimistic, and the expected errors underestimate the true errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:improving_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we develop more sophisticated Gaussian process models than the simple product of squared exponential kernels. We then assess whether these models improve the variogram and the predictive diagnostic measures that we developed in the previous sections.\n",
    "\n",
    "The most salient feature of the empirical variogram that isn't captured by the `SExSE` model is the oscillation with a 24-hour period. It is intuitively obvious that the diurnal cycle induces this periodic covariance, and that our model should be improved by incorporating this feature. Gaussian process models allow for periodic components of the covariance, for example the periodic squared exponential kernel, which we will use with a 24-hour period\n",
    "\\begin{equation}\n",
    "    k_{24}(t,t') = \\sigma_{24}^2 \\exp\\sbr{ - \\frac{2}{\\ell_{24}^2} \\sin^2\\del{\n",
    "        \\pi \\frac{t-t'}{\\text{24 hrs}} \n",
    "        }}\\,.\n",
    "\\end{equation}\n",
    "We modify the spatiotemporal model by adding this diurnal component to it, with its own spatial decay kernel $k_{space24}$ (with the same specification as $k_{space}$ in \\eqref{eq:kspace}).\n",
    "\\begin{equation}\n",
    "    \\kdiurn(\\xvec,\\xvec',t,t') = k_{time}(t,t') \\cdot k_{space}(\\xvec, \\xvec') \n",
    "        + k_{24}(t,t') \\cdot k_{space24}(\\xvec, \\xvec')\n",
    "        + k_\\mu(\\xvec, \\xvec') \n",
    "        \\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also develop a more complex model, which breaks up $k_{time}$ into short-term, medium-term and long-term correlation components, each with their own spatial decay.\n",
    "\n",
    "\\begin{equation}\n",
    "\\eqlabel{eq:sumprod_kernel}\n",
    "\\begin{aligned}\n",
    "    \\ksumprod(\\xvec,\\xvec',t,t') &= \n",
    "           k_{time1}(t,t') \\cdot k_{space1}(\\xvec, \\xvec')  &\\text{(short-term variation)} \\\\\n",
    "        &+ k_{time2}(t,t') \\cdot k_{space2}(\\xvec, \\xvec')  &\\text{(medium-term variation)} \\\\\n",
    "        &+ k_{time3}(t,t') \\cdot k_{space3}(\\xvec, \\xvec')  &\\text{(long-term variation)} \\\\\n",
    "        &+ k_{24}(t,t') \\cdot k_{space24}(\\xvec, \\xvec') &\\text{(diurnal cycle)} \\\\\n",
    "        &+ k_\\mu(\\xvec, \\xvec') &\\text{(station mean)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Each of $k_{time1}$, $k_{time2}$, and $k_{time3}$, is a rational quadratic kernel\n",
    "\\begin{equation}\n",
    "    k_{RQ}(t,t') = \\sigma^2 \\del{1 + \\frac{\\del{t-t'}^2}{2\\alpha\\ell^2} }^{-\\alpha}\n",
    "\\end{equation}\n",
    "which is accompanied by its spatial decay kernel, specified as a squared exponential covariance.\n",
    "This more complicated kernel therefore has $3 \\times 3 \\times 2 + 2 \\times 2 = 22$ free parameters, in addition to the noise parameter $\\sigman^2$.\n",
    "\n",
    "We now have three competing Gaussian process models, with covariance functions $\\kSESE$, $\\kdiurn$, and $\\ksumprod$ respectively. We can compare them in three ways. Firstly, the marginal log-likelihood is the quantity maximized by the parameter fitting procedure in \\eqref{eq:optimization}. The maximized log-likelihood can be found in the second column of Table  \\ref{table:diagnostics}, and we see that the more complex models indeed yield a much higher log-likelihood, promising a better model fit which should yield better predictions. Secondly, we compare the variance of the error in the predicted temperatures specified in \\eqref{eq:varerr} when withholding all the data from a test station. Averaged over all of 2015, this is given in the third column, and shows more mixed results. The diurnal model $\\kdiurn$ performs worse than the simple $\\kSESE$ model, and $\\ksumprod$ only yields a small improvement. \n",
    "Thirdly, we can reintroduce the daily minima and maxima from the withheld station, and compare the mean squared error specified in \\eqref{eq:mse} for predictions at the test station. Results in the fifth column show even more modest improvements for the more complex models.\n",
    "\n",
    "We interpret these results as a reminder that predictions using Gaussian process are sensitive to model specification when extrapolating, but fairly insensitive to the model when interpolating <cite data-cite=\"stein2012interpolation\">(Stein, 2012)</cite>. Since our imputations interpolate the temperatures from nearby stations, further aided by the constraints imposed by the daily $\\Tn$ and $\\Tx$ measurements, the choice of model does not have a large impact on the performance of our procedure. This insensitivity can be seen as reassuring, as it (to an extent) reduces our need to worry about the incorrectness of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputed summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure \\ref{fig:imputations_2x2}(d) shows the imputations produced under the $\\ksumprod$ kernel \\eqref{eq:sumprod_kernel}.\n",
    "This is the primary output of our imputation method, and it shows promising behavior.\n",
    "Firstly, just like in the toy example presented in \\ref{sec:toy_example}, the individual imputations meet the three constraints imposed by the measured minimum and maximum.\n",
    "Each day, the imputations stay between $\\Tn$ and $\\Tx$,\n",
    "and the temperatures always drop to $\\Tn$ and rise to $\\Tx$ at some time of the day.\n",
    "The imputations reflect the uncertainty in the time at which the extrema are reached.\n",
    "Notably, on some days, the posterior distribution of the warmest (or coldest) time is bimodal.\n",
    "For example this can be seen on May 31st, where some imputations reach $\\Tx$ at the start of the measurement window, while others reach it at the end.\n",
    "We view as a particular strength of our approach that the imputations are able to capture this bimodality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:imputed_summary_stats}](../figures/imputed_summary_stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These imputations however are not the final aim of an analysis.\n",
    "Rather, our stated goal is to undo, or at least account for, the sensitivity of summary statistics to measurement time, for example the average $\\Tx$ in Figure \\ref{fig:waterloo_avgTnTx}.\n",
    "Equipped with these imputations, is it possible to infer what the value of the summary statistic would have been for different measurement hours?\n",
    "This capability is demonstrated in Figure \\ref{fig:imputed_summary_stats},\n",
    "which shows the same summary statistic as in Figure \\ref{fig:waterloo_avgTnTx} applied to the individual imputations instead of the (witheld) hourly data at Waterloo Airport.\n",
    "The thick orange line shows the mean of the summary statistic over all imputations, while the envelope is $\\pm 2$ standard deviations.\n",
    "**[need to explain how posterior variance is obtained]**\n",
    "It can be seen that the imputed summary statistics track within about 0.1 $\\degreeC$ of the true values,\n",
    "while the posterior standard deviation gives a fair estimate of the imputation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on measurement hour\n",
    "\n",
    "Our analysis thus far has focused on the case where the hour of measurement $\\hour$ is known in advance.\n",
    "This is an unrealistic assumption in practice, and so inference on $\\hour$ is a desirable feature.\n",
    "It is conceptually straightforward to modify the measurement model \\eqref{eq:smoothed_model} with a uniform prior on $\\hour$.\n",
    "However, because we obtain our imputations in ten-day windows, in most windows precise information about $\\hour$ will not be available, as moving the measurement time one hour earlier or later rarely affects the measured $\\Tn$ and $\\Tx$.\n",
    "Furthermore, $\\hour$ affects which observations are attributed to each day's measurements. \n",
    "This effect is discontinuous (observations suddenly jump from one day to the next) and non-differentiable, and so Hamiltonian Monte Carlo becomes unviable.\n",
    "This issue is similar to that caused by the non-differentiability of the minimum and maximum functions.\n",
    "We therefore do not consider the introduction of a uniform prior on $\\hour$ in Stan to be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our procedure allows us to obtain imputation samples of $\\T_\\miss$ conditional on $\\T_\\obs,\\Tn,\\Tx$ and $\\hour$. \n",
    "If we do so for $\\hour=1,2,\\ldots,24$, is there information available in these samples to infer $\\hour$?\n",
    "We will examine sample imputations to answer this question.\n",
    "Figure \\ref{fig:measure_hour_example} shows mean imputation for temperatures over nine days starting on February 27, 2015. The orange line is the mean using only nearby temperatures (shifted by a constant to match the true temperatures), while the green line is additionally conditional on $\\Tn$ and $\\Tx$; the true temperatures are shown in grey.\n",
    "The top plot shows the imputation under the correct daily measurement time (17 UTC), while the bottom plot is under an incorrect measurement time (5 UTC).\n",
    "The first unsurprising observation is that assuming an incorrect measurement time can lead to wildly inaccurate imputations.\n",
    "But we then also notice that assuming the wrong time also causes the mean constrained imputation to depart further from the unconstrained imputation \n",
    "(that is, the green and orange lines are further apart).\n",
    "This can be interpreted as an indication of an incompatibility between $\\T_\\obs$ and the daily extremes, caused by assuming the wrong $\\hour$.\n",
    "To quantify this discrepancy, we propose to calculate the probability of the mean constrained imputation under the unconstrained posterior given by $\\eqref{eq:unconstrained_post}$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\mu\\del{\\hour} &\\equiv \\E\\del{ \\T_\\miss \\mid \\T_\\obs, \\Tn, \\Tx, \\hour } \\text{ (the mean imputed temperature), }\\\\\n",
    "    \\discrepancy_\\hour &\\equiv \\Pr\\del{ \\T_\\miss = \\mu\\del{\\hour} \\mid \\T_\\obs }\\,.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Our intuition is that $\\discrepancy_\\hour$ will drop sharply when the wrong $\\hour$ is assumed,\n",
    "and we may be able to infer the true $\\hour$ by maximizing $\\discrepancy_\\hour$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:measure_hour_example} A sample window showing constrained and unconstrained imputations assuming (top) the correct measurement hour (17 UTC), and (bottom) the wrong measurement hour (5 UTC). Assuming the wrong measurement time drives the constrained mean imputation away from the unconstrained mean imputation.](../figures/measure_hour_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:hr_inference} Discrepancy measure for imputations of temperatures at Waterloo Municipal Airport assuming measurement hours $\\hour=1,2,\\ldots,24$. The true hour of measurement is 17, and obtains the highest $\\delta_\\hour$.](../figures/hr_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Climatological research relies on the ability to track small changes over long periods. \n",
    "For this reason, the subtle bias induced by the measurement time \n",
    "exposed by <cite data-cite=\"baker1975effect\">(Baker, 1975)</cite> \n",
    "could lead to wrong estimates and conclusions regarding long-term trends in temperature records. \n",
    "In this paper, this is illustrated in section \\ref{sec:illustrate_bias}, \n",
    "which demonstrates using temperature records from a meteorological station in Iowa, that the answer to a simple question such as “what was the average daily maximum temperature in 2015?” can change by about 1$\\degreeC$ depending on the time of measurement.\n",
    "A drift in the time of measurement from one year to the next could therefore result in misleading conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reframed the source of the bias as a missing data problem.\n",
    "If we had access to the full time series of temperatures, we could choose retrospectively to emulate the measurements of daily extremes at any time of day.\n",
    "For example, we may wish to emulate measurements of $\\Tx$ at an hour that is typically cold, and $\\Tn$ at a warm hour, in order to maximize our chances of measuring the true peaks of each diurnal cycle.\n",
    "This missing data perspective motivated our approach, which is to impute the temperature time series, by combining the information available from the record of daily temperature extremes, \n",
    "and from nearby records of hourly temperatures.\n",
    "\n",
    "The hourly records are typically at meteorological stations\n",
    "that aren't always sufficiently well-calibrated\n",
    "or their equipment and procedures sufficiently well-documented\n",
    "to be directly used for climate science.\n",
    "Often they are located near airports and urban areas,\n",
    "which further affects the trustworthiness of their data.\n",
    "This precludes directly relying on meteorological records to estimate climatological trends.\n",
    "However, even if the hourly records are miscalibrated and biased,\n",
    "they can tell us how the temperature changed over the course of a day.\n",
    "This is the information that is missing from daily measurements of temperature extremes made at research-quality weather stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore have two pieces of the puzzle — shape and daily extremes of the time series — but they are obtained at separate locations.\n",
    "To combine them, we needed a spatio-temporal model, and proposed a Gaussian process with squared exponential kernels \\eqref{eq:gpmodel} in space and time.\n",
    "While this is not a realistic physical model, it is very flexible, and is generally adequate for the task of interpolating temperatures between nearby locations and times.\n",
    "Misspecification of spatial models is much more troublesome for extrapolation tasks <cite data-cite=\"stein2012interpolation\">(Stein, 2012)</cite>.\n",
    "Indeed, more complex kernels (with a diurnal component and a sum of short-range and long-range components) showed only modest improvements in the mean squared error of the imputations compared to a withheld hourly temperature record.\n",
    "\n",
    "The $\\GP$ specification and multivariate normal theory allow us to derive analytical predictions \\eqref{eq:unconstrained_post} of temperatures at the location of a climatological station given hourly measurements from nearby meteorological stations.\n",
    "Each location also has a mean parameter which is given an independent vague prior, with no spatial correlation.\n",
    "This is because many of the hourly temperature measurements are made near airports, or the instruments may be miscalibrated, and we do not wish these systematic effects to affect the imputations.\n",
    "The $\\GP$ model therefore only makes predictions at new locations up to a constant shift, and it only extracts information about the trajectory of the temperature time series from each weather station.\n",
    "The assumption underlying our model is that this trajectory is less strongly affected by nearby human activity and calibration error, and can therefore be used without introducing undue biases in the imputations.\n",
    "This assumption is violated for example if the presence of an airport has a very different effect on measured temperatures during the day and during the night, which would introduce some bias in the imputations.\n",
    "\n",
    "In order to further condition the predictions on the taily $\\Tx$ and $\\Tn$, we developed SmoothHMC, a general algorithm based on Hamiltonian Monte Carlo with a smoothed approximation of the target distribution that can sample from a multivariate distribution conditionally on its observed minimum and maximum.\n",
    "It showed an excellent ability to sample from the conditional distribution in an example where the distribution function can also be obtained analytically.\n",
    "SmoothHMC is the main technical contribution of this paper, and we believe the method could find applications beyond the present setting.\n",
    "\n",
    "We used this method to obtained imputations of the temperature time series that satisfied the constraints imposed by the measured $\\Tn$ and $\\Tx$.\n",
    "The imputation of withheld temperatures at Waterloo Municipal Airport tracked the true temperatures closely.\n",
    "We view as particularly encouraging that the imputations successfully capture bimodalities of the time of the maximum or minimum temperature on days where this time is difficult to infer from the available information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imputed time series are the primary output of this work, but they are intended as a starting point for further analyses motivated by different scientific goals.\n",
    "In particular, summary statistics can be applied to the imputations.\n",
    "For example, one can now answer questions such as “what would the average $\\Tx$ have been in 2015 if the measurements had instead been made at midnight?” by extracting and averaging daily maxima from each imputation, for 24 hour periods starting and ending at midnight each day.\n",
    "Using imputations obtained for the withheld time series at Waterloo airport, we have demonstrated a good ability to recover this information, though the resulting posterior variance seems to underestimate the error.\n",
    "\n",
    "The average $\\Tx$ is an example of a possible follow-up analysis, chosen mostly as an illustrative proof of concept.\n",
    "We plan to use this method to compare the average temperature to the average of the measured $\\Tn$ and $\\Tx$ for a given location and year, with the former obtained from imputed time series."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stan programs\n",
    "\n",
    "## Smoothmax toy example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:stan_illustration}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without `smoothmax` Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```stan   \n",
    "data {\n",
    "    int<lower=0> N; // number of observations\n",
    "    real Xmax;\n",
    "    real Xmin;\n",
    "    vector[N] mu_i;\n",
    "    real<lower=0> sigma_i[N];\n",
    "}\n",
    "parameters {\n",
    "    vector[N] X_i; // latent variables\n",
    "}\n",
    "model {\n",
    "    X_i ~ normal(mu_i, sigma_i);\n",
    "    Xmax ~ normal(max(X_i), 0.01);\n",
    "    Xmin ~ normal(min(X_i), 0.01);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `smoothmax` Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```stan\n",
    "functions {\n",
    "    real smoothmax(vector x, real k, real maxkx){\n",
    "        return (maxkx+log(sum(exp(k*x - maxkx))))/k;\n",
    "    }\n",
    "    real smoothmin(vector x, real k, real minkx){\n",
    "        return -smoothmax(-x, k, -minkx);\n",
    "    }\n",
    "}\n",
    "data {\n",
    "    int<lower=0> N; // number of observations\n",
    "    real Xmax;\n",
    "    real Xmin;\n",
    "    real mu_i[N];\n",
    "    real<lower=0> sigma_i[N];\n",
    "    real<lower=0> k;\n",
    "}\n",
    "parameters {\n",
    "    vector[N] X_i; // latent variables\n",
    "}\n",
    "transformed parameters {\n",
    "    real Xsmoothmax;\n",
    "    real Xsmoothmin;\n",
    "    Xsmoothmax = smoothmax(X_i, k, k*Xmax);\n",
    "    Xsmoothmin = smoothmin(X_i, k, k*Xmin);\n",
    "}\n",
    "model {\n",
    "    X_i ~ normal(mu_i, sigma_i);\n",
    "    Xmax ~ normal(Xsmoothmax, 0.01);\n",
    "    Xmin ~ normal(Xsmoothmin, 0.01);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature imputations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:appendix_stan}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "functions {\n",
    "    real smoothmax(vector x, real k, real maxkx){\n",
    "        return (maxkx+log(sum(exp(k*x - maxkx))))/k;\n",
    "    }\n",
    "    real smoothmin(vector x, real k, real minkx){\n",
    "        return -smoothmax(-x, k, -minkx);\n",
    "    }\n",
    "}\n",
    "data {\n",
    "    // Tn Tx data\n",
    "    int<lower=1> N_TxTn; //\n",
    "    vector[N_TxTn] Tx;\n",
    "    vector[N_TxTn] Tn;\n",
    "\n",
    "    // imputation points (for which we have )\n",
    "    int<lower=1> Nimpt;\n",
    "    int<lower=1,upper=N_TxTn> day_impute[Nimpt];\n",
    "    // number of hours recorded within each day\n",
    "    int<lower=1> impt_times_p_day[N_TxTn];\n",
    "\n",
    "    // prior \n",
    "    vector[Nimpt] predicted_mean;\n",
    "    matrix[Nimpt,Nimpt] predicted_cov;\n",
    "    matrix[Nimpt,Nimpt] predicted_cov_chol;\n",
    "\n",
    "    // control soft max hardness\n",
    "    real<lower=0> k_smoothmax;\n",
    "}\n",
    "parameters {\n",
    "    vector[Nimpt] w_uncorr;\n",
    "    real mu;\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[Nimpt] temp_impt;\n",
    "    real Tsmoothmax[N_TxTn];\n",
    "    real Tsmoothmin[N_TxTn];  \n",
    "    temp_impt = mu + predicted_mean + predicted_cov_chol*w_uncorr;\n",
    "    {\n",
    "        int istart;\n",
    "        istart = 1;\n",
    "        for (i in 1:N_TxTn){\n",
    "            int ntimes;\n",
    "            ntimes = impt_times_p_day[i];\n",
    "            Tsoftmin[i] = smoothmin(segment(temp_impt,istart,ntimes), \n",
    "                                    k_smoothmax, \n",
    "                                    k_smoothmax*Tn[i]);\n",
    "            Tsoftmax[i] = smoothmax(segment(temp_impt,istart,ntimes), \n",
    "                                    k_smoothmax,\n",
    "                                    k_smoothmax*Tx[i]);\n",
    "            istart = istart + ntimes;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "model {\n",
    "    w_uncorr ~ normal(0,1);\n",
    "    mu ~ normal(0, 100.0);\n",
    "    Tn ~ normal(Tsmoothmin, 0.1);\n",
    "    Tx ~ normal(Tsmoothmax, 0.1);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation of the analytic posterior for toy example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:analytical_posterior}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first derive and compute $\\Fcond$ for this example.\n",
    "We denote by $f_i(\\cdot)$ and $F_i(\\cdot)$ the prior probability distribution function and cumulative distribution function of $X_i$, i.e. the normal PDF and CDF with means and variances given by \\eqref{eq:toyspec}.\n",
    "Let $\\pij$ be the probability that $X_i$ is the minimum of $X$, and $X_j$ is its maximum.\n",
    "We also define $\\pisum = \\sum_{j=1}^{100} \\pij$, the probability that $X_i$ is the minimum,\n",
    "and $\\psumj = \\sum_{i=1}^{100} \\pij$, the probability that $X_j$ is the maximum.\n",
    "The cumulative distribution function of $X_i$ is then given by\n",
    "\\begin{equation}\n",
    "\\Pr\\del{X_i \\leq x \\mid \\Xmax, \\Xmin} =\n",
    "    \\begin{cases}\n",
    "        0 &\\text{when } x < \\Xmin \\,, \\\\\n",
    "        \\pxx{i}{\\bullet} \n",
    "            + \\del{1 - \\pxx{i}{\\bullet} - \\pxx{\\bullet}{i}}\n",
    "            \\sbr{\\frac{F_i(x) - F_i(\\Xmin) }\n",
    "                 {F_i(\\Xmax) - F_i(\\Xmin) }\n",
    "                } \n",
    "            &\\text{when } \\Xmin \\leq x < \\Xmax \\,, \\\\\n",
    "        1 &\\text{when } x \\geq \\Xmax \\,.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "Meanwhile, $\\pij$ is proportional to\n",
    "\\begin{equation}\n",
    "    f_i(\\Xmin)\n",
    "    f_j(\\Xmax)\n",
    "    \\prod_{k \\neq i,j}^{100}\n",
    "    \\del{F_k(\\Xmax) - F_k(\\Xmin)} \\,,\n",
    "\\end{equation}\n",
    "which we compute for all $i,j$ and renormalize\n",
    "to obtain the $100 \\times 100$ matrix of probabilities.\n",
    "We sum over its rows and columns to obtain $\\psumj$ and $\\pisum$.\n",
    "While this algorithm has cubic complexity in the dimensionality $p$ of $X$,\n",
    "for $p=100$ \n",
    "computers only take seconds to compute the entries of $\\Pr$ and evaluate $\\Pr\\del{X_i \\leq x \\mid \\Xmax, \\Xmin}$ over a range of $x$.\n",
    "Figure \\ref{fig:toy_quantiles}(b) shows the analytical quantiles of $\\Fcond$.\n",
    "Roughly speaking, we see that the prior distribution $F_X$ is stretched to fit between $\\Xmin$ and $\\Xmax$."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": false,
   "bibliofile": "temperature_imputations.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 5,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "246px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "996px",
    "left": "0px",
    "right": "auto",
    "top": "66px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
