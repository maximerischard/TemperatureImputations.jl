{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev2 toc-item\"><a href=\"#Illustration-of-bias-induces-by-measurement-hour\" data-toc-modified-id=\"Illustration-of-bias-induces-by-measurement-hour-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Illustration of bias induces by measurement hour</a></div><div class=\"lev2 toc-item\"><a href=\"#Proposed-solution\" data-toc-modified-id=\"Proposed-solution-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Proposed solution</a></div><div class=\"lev1 toc-item\"><a href=\"#First-Spatiotemporal-Model\" data-toc-modified-id=\"First-Spatiotemporal-Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>First Spatiotemporal Model</a></div><div class=\"lev2 toc-item\"><a href=\"#Fitting-the-spatiotemporal-model\" data-toc-modified-id=\"Fitting-the-spatiotemporal-model-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Fitting the spatiotemporal model</a></div><div class=\"lev1 toc-item\"><a href=\"#Predictions-using-nearby-data\" data-toc-modified-id=\"Predictions-using-nearby-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predictions using nearby data</a></div><div class=\"lev1 toc-item\"><a href=\"#Imputations\" data-toc-modified-id=\"Imputations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Imputations</a></div><div class=\"lev2 toc-item\"><a href=\"#Imputing-by-Conditioning-on-Extrema\" data-toc-modified-id=\"Imputing-by-Conditioning-on-Extrema-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Imputing by Conditioning on Extrema</a></div><div class=\"lev2 toc-item\"><a href=\"#Illustration-of-Hamiltonian-Monte-Carlo-with-Smoothmax-Approximation\" data-toc-modified-id=\"Illustration-of-Hamiltonian-Monte-Carlo-with-Smoothmax-Approximation-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Illustration of Hamiltonian Monte Carlo with Smoothmax Approximation</a></div><div class=\"lev2 toc-item\"><a href=\"#Smoothmax-Temperature-Model\" data-toc-modified-id=\"Smoothmax-Temperature-Model-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Smoothmax Temperature Model</a></div><div class=\"lev1 toc-item\"><a href=\"#Model-diagnostics\" data-toc-modified-id=\"Model-diagnostics-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model diagnostics</a></div><div class=\"lev2 toc-item\"><a href=\"#Variogram\" data-toc-modified-id=\"Variogram-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Variogram</a></div><div class=\"lev2 toc-item\"><a href=\"#Error-and-expected-error\" data-toc-modified-id=\"Error-and-expected-error-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Error and expected error</a></div><div class=\"lev1 toc-item\"><a href=\"#Improving-model\" data-toc-modified-id=\"Improving-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Improving model</a></div><div class=\"lev1 toc-item\"><a href=\"#Analysis\" data-toc-modified-id=\"Analysis-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Analysis</a></div><div class=\"lev1 toc-item\"><a href=\"#Inference-on-measurement-hour\" data-toc-modified-id=\"Inference-on-measurement-hour-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Inference on measurement hour</a></div><div class=\"lev1 toc-item\"><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Appendices</a></div><div class=\"lev2 toc-item\"><a href=\"#Stan-programs-for-illustration-of-smoothmax\" data-toc-modified-id=\"Stan-programs-for-illustration-of-smoothmax-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Stan programs for illustration of smoothmax</a></div><div class=\"lev3 toc-item\"><a href=\"#Without-smoothmax-Approximation\" data-toc-modified-id=\"Without-smoothmax-Approximation-911\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>Without <code>smoothmax</code> Approximation</a></div><div class=\"lev3 toc-item\"><a href=\"#With-smoothmax-Approximation\" data-toc-modified-id=\"With-smoothmax-Approximation-912\"><span class=\"toc-item-num\">9.1.2&nbsp;&nbsp;</span>With <code>smoothmax</code> Approximation</a></div><div class=\"lev1 toc-item\"><a href=\"#Stan-model-for-temperature-imputations\" data-toc-modified-id=\"Stan-model-for-temperature-imputations-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Stan model for temperature imputations</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\require{cancel}\n",
    "    \\newcommand{\\genericdel}[3]{%\n",
    "      \\left#1#3\\right#2\n",
    "    }\n",
    "    \\newcommand{\\del}[1]{\\genericdel(){#1}}\n",
    "    \\newcommand{\\sbr}[1]{\\genericdel[]{#1}}\n",
    "    \\newcommand{\\cbr}[1]{\\genericdel\\{\\}{#1}}\n",
    "    \\newcommand{\\abs}[1]{\\genericdel||{#1}}\n",
    "    \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "    \\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "    \\DeclareMathOperator{\\Pr}{\\mathbb{p}}\n",
    "    \\DeclareMathOperator{\\E}{\\mathbb{E}}\n",
    "    \\DeclareMathOperator{\\Ind}{\\mathbb{I}}\n",
    "    \\DeclareMathOperator{\\var}{Var}\n",
    "    \\DeclareMathOperator{\\mse}{MSE}\n",
    "    \\DeclareMathOperator{\\cov}{Cov}\n",
    "    \\DeclareMathOperator{\\invchi}{\\mathrm{Inv-\\chi}^2}\n",
    "    \\newcommand{\\effect}{\\mathrm{eff}}\n",
    "    \\newcommand{\\xtilde}{\\widetilde{X}}\n",
    "    \\DeclareMathOperator{\\normal}{\\mathcal{N}}\n",
    "    \\DeclareMathOperator{\\unif}{Uniform}\n",
    "    \\DeclareMathOperator{\\GP}{\\mathcal{GP}}\n",
    "    \\newcommand{\\T}{ {T} }\n",
    "    \\newcommand{\\Tn}{\\T_{n}}\n",
    "    \\newcommand{\\Tx}{\\T_{x}}\n",
    "    \\newcommand{\\station}[1]{\\mathrm{station}\\sbr{#1}}\n",
    "    \\newcommand{\\xvec}{\\mathbf{x}}\n",
    "    \\newcommand{\\hvec}{\\mathbf{h}}\n",
    "    \\newcommand{\\indep}{\\perp}\n",
    "    \\newcommand{\\iid}{iid}\n",
    "    \\newcommand{\\trans}{^{\\intercal}}\n",
    "    \\newcommand{\\sigmaf}{\\sigma_{\\mathrm{GP}}}\n",
    "    \\newcommand{\\sigman}{\\sigma_{\\epsilon}}\n",
    "    \\newcommand{\\degreeC}{^\\circ \\mathrm{C}}\n",
    "    \\newcommand{\\miss}{\\mathrm{miss}}\n",
    "    \\newcommand{\\obs}{\\mathrm{nearby}}\n",
    "    \\DeclareMathOperator*{\\softmax}{smoothmax}\n",
    "    \\DeclareMathOperator*{\\softmin}{smoothmin}\n",
    "    \\newcommand{\\error}{\\mathrm{err}}\n",
    "    \\newcommand{\\hour}{ {hr} }\n",
    "    \\DeclareMathOperator{\\kSESE}{k_{\\mathtt{SExSE}}}\n",
    "    \\DeclareMathOperator{\\kdiurn}{k_{\\mathtt{SESE\\_24}}}\n",
    "    \\DeclareMathOperator{\\ksumprod}{k_{\\mathtt{sumprod}}}\n",
    "    \\newcommand{\\iday}{\\mathtt{day}}\n",
    "    \\newcommand{\\dayset}[1]{\\cbr{i}_{#1}}\n",
    "    \\newcommand{\\discrepancy}{\\delta}\n",
    "    \\newcommand{\\Xmax}{X_{\\max}}\n",
    "    \\newcommand{\\Xmin}{X_{\\min}}\n",
    "    \\newcommand{\\Fcond}{F_{X \\mid \\Xmax,\\Xmin}}\n",
    "    \\newcommand{\\pxx}[2]{\\Pr{}_{#1#2}}\n",
    "    \\newcommand{\\pij}{\\pxx{i}{j}}\n",
    "    \\newcommand{\\pisum}{\\pxx{i}{\\bullet}}\n",
    "    \\newcommand{\\psumj}{\\pxx{\\bullet}{j}}\n",
    "    \\newcommand{\\eqlabel}[1]{}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2\n",
      "16.1\n"
     ]
    }
   ],
   "source": [
    "begin\n",
    "    hr = Hour(23)\n",
    "    low = Hour(8)\n",
    "    peak = Hour(20)\n",
    "    TnTx_low = waterloo_TnTx_dict[low]\n",
    "    TnTx_peak = waterloo_TnTx_dict[peak]\n",
    "    \n",
    "    apr3_low = TnTx_low[TnTx_low[:ts_day].values.==DateTime(2015,4,4),:]\n",
    "    apr3_peak = TnTx_peak[TnTx_peak[:ts_day].values.==DateTime(2015,4,4),:]\n",
    "    \n",
    "    global apr3_realmax = get(apr3_low[1,:Tx])\n",
    "    global apr3_measured = get(apr3_peak[1,:Tx])\n",
    "    println(apr3_realmax)\n",
    "    println(apr3_measured)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Long, high-quality records of temperature provide an important basis for our understanding of climate variability and change. Historically, there has been a focus on monthly-average temperature records, which are sufficient for certain analyses, such as quantifying long-term changes in temperature. As our knowledge of climate change expands, however, there is increasing interest in understanding changes in temperature on shorter timescales, with a particular focus on extreme events. To do so, it is necessary to utilize higher-resolution temperature data. \n",
    "\n",
    "Recent work has led to the development of the Global Historical Climatology Network-Daily (GHCND) database \\citep{menne2012overview}, which contains, among other variables, daily maximum and minimum temperatures from weather stations around the globe. The database draws from a range of different sources, and the data within it undergoes basic quality control to remove erroneous values. \n",
    "\n",
    "The current quality control methodology, however, does not account for so-called `inhomogeneities'. Inhomogeneities result from changes in measurement practices that impact the recorded temperatures. For temperature, known inhomogeneities include (a) changes in the time of observation, (b) changes in the thermometer technology, (c) station relocation, and (d) changes in land use around a station \\citep{menne2009us}. While these inhomogeneities have a small effect on, e.g., the estimation of global mean temperature, they can have a large effect on estimation of temperature variability and change at a more local scale.\n",
    "\n",
    "There is a large body of work focused on homogenizing monthly-average temperatures \\citep[e.g.,][]{karl1986model, easterling1996development, peterson1998homogeneity, ducre2003comparison, menne2009homogenization, vincent2012second}, resulting in widely available, large-scale homogenized monthly temperature datasets. Homogenization typically proceeds through identifying non-climatic `breakpoints' in a given time series through comparison with neighboring stations. Once a breakpoint is identified, the measurements recorded after the breakpoint are adjusted in some way to reduce or remove the inhomogeneity. Most applications of these methods, however, focus on adjusting the mean state of the data rather than the shape of the distribution \\citep[see][and references therein]{della2006method}. While this may be sufficient for monthly data, it is known that certain changes in measurement practices affect different percentiles of daily temperature in different ways. To address this issue, some homogenization methods have also employed percentile matching techniques, wherein the adjustment to a timeseries after a breakpoint is a function of percentile \\citep{della2006method, trewin2013daily}. \n",
    "\n",
    "Here, we focus primarily on addressing the time of observation bias, as well as its time trend, its because of its known impact on the distribution of daily maximum and minimum temperature (Tx and Tn) measurements. The bias exists because Tx and Tn are often recorded by an observer who visits a weather station every 24 hours, and notes the maximum and minimum temperatures measured by the thermometer over the previous 24 hours. Ideally, the observer would visit the station at midnight, and the highest and lowest temperatures over the past 24 hours would typically be representative of the high and low during the prior day. For convenience, however, most observers record data at a daytime hour instead. As can be seen in Fig.~XX, measurements recorded in the early morning may not properly register the low of the night before if it was unusually warm. Similarly, measurements recorded in the late afternoon may not properly register the high of the prior day if it was usually cool. In both cases, this will lead to a reduction in the variance of Tx and Tn distributions, but the effect will be greater at low (high) percentiles for Tx (Tn). \n",
    "\n",
    "If the time of observation remained constant over time, the bias would still exist, but it would not be linked to spurious trends in the data. However, there have been known (and likely unknown) changes in the time of observation. In the United States, for example, observers were instructed to switch from recording data in the afternoon to recording data in the morning beginning in the 1950s. This change has led to an apparent decrease in both Tx and Tn over time \\citep{menne2009us}. \n",
    "\n",
    "The goal of our approach is to infer the true $\\Tn$ and $\\Tx$ values throughout the data records, thereby correcting both the variance biases and the spurious trends. This stands in constrast to previous work, which has focused primarily on addressing spurious trends. We approach the problem as a missing data problem, wherein we are trying to recover the values of Tx and Tn that may have been overwritten due to measurement practices. Furthermore, by employing a Gaussian process framework and nearby stations with hourly data, we are able to simulate multiple realizations of temperature timeseries at each station, thereby providing estimates of uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of bias induces by measurement hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cite data-cite=\"baker1975effect\">(Baker, 1975)</cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the measurement bias in daily maxima and minima with ten days of hourly temperature measurements from the Waterloo Municipal Airport station in Iowa. \n",
    "Ideally, $\\Tx$ measurements should capture the peak of each diurnal cycle, and $\\Tn$ its trough. \n",
    "In Figure \\ref{fig:waterloo_triangles}, those ideal measurements are indicated by the red and blue triangles respectively. \n",
    "The actual measurements are obtained by dividing the data into 24 hour measurement windows, and extracting the minimum and maximum. \n",
    "For each window, we plot these extrema with a red and blue horizontal line.\n",
    "\n",
    "![\\label{fig:waterloo_triangles}](figures/waterloo_triangles.png)\n",
    "\n",
    "On most days, the ideal measurement and the actual measurement coincide: the triangle is on that day's line. \n",
    "But there are also several misses. The most blatant example occurs on April 3rd, \n",
    "where the peak of the diurnal cycle is 7.2°C and occurs at 21:00 UTC. \n",
    "However, because the previous day was much warmer, the day's $\\Tx$ record of 16.1°C is reached immediately after the previous day's measurement. \n",
    "The measured $\\Tx$ therefore overestimates the diurnal cycle's peak by 8.9°C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subtle bias in the daily records can in turn bias long-term summary statistics that are of climatological interest. \n",
    "A measure as simple as the average daily maximum temperature for an entire year (2015) increases by over 1°C if the measurements are made at the warmest time of day 21:00 UTC rather than 14:00 UTC (see Figure \\ref{fig:waterloo_avgTnTx}).\n",
    "Conversely, the average $\\Tn$ is colder by over 1°C if $\\Tn$ is measured at 10:00 UTC (the coldest time of day on average) rather than 17:00 UTC.\n",
    "\n",
    "![\\label{fig:waterloo_avgTnTx}](figures/waterloo_avgTnTx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A climatologist studying weather variability might be interested in summary statistics such as the average absolute change in the daily temperature maxima and minima from one day to the next.\n",
    "The answer to that question too depends on the time of day at which the temperatures are recorded.\n",
    "Collecting the measurements at the hottest time of day means that the peaks on a warm day gets recorded twice, \n",
    "erasing the diurnal peaks of the following colder day, \n",
    "and hence the variability gets underestimated.\n",
    "We can see this in Figure \\ref{fig:waterloo_meanabsdiff}), where the respective variability estimates drop if the maxima get measured at the warmest time, or if the minima get measured at the coldest time.\n",
    "\n",
    "![\\label{fig:waterloo_meanabsdiff}](figures/waterloo_meanabsdiff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed solution\n",
    "\n",
    "We have seen that the daily maxima and minima do not faithfully record each diurnal cycle's peak and trough. \n",
    "The peaks on a relatively cold day can get overwritten by temperatures at either end of the measurement window that properly belong to the previous or the next diurnal cycle. \n",
    "Troughs on relatively warm days can be similarly overwritten. \n",
    "Our goal is to undo this damage, and recover estimates of summary statistics, such as the average daily maximum temperature, that do not suffer from the consequent bias. \n",
    "We need to address the erasure of information caused by the measurement mechanism, and therefore view this as a missing data problem.\n",
    "\n",
    "Taking the missing data perspective, we seek to impute the hourly temperatures that have been replaced by a maximum and minimum over a 24 hour period. \n",
    "To do so, we combine information from two sources: the recorded daily temperature extremes at the station of interest, and also hourly temperatures recorded at nearby meteorological stations. \n",
    "These hourly measurements are considered less reliable by climatologists, as they aren't as carefully documented, calibrated, and situated. \n",
    "The meterological stations are often in locations (like airports) where human activity will affect temperatures.\n",
    "Therefore, summary statistics extracted directly from those measurements would not be directly usable for climatology, as they could suffer from systematic bias.\n",
    "However, even if miscalibrated, the meterological data do contain valuable information about the hourly changes in temperatures on any given day.\n",
    "We therefore use them to inform the shape of the imputed temperature time-series at our location of interest,\n",
    "while we use the recorded temperature extrema to calibrate and constrain them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Spatiotemporal Model\n",
    "\n",
    "To model measured temperatures at various locations and times, we use a spatio-temporal Gaussian process model. \n",
    "In its simplest form, we believe that temperatures from stations that are near each other are more correlated than distant stations, and that those correlations should also decay in time. \n",
    "In the spatial statistics literature, squared exponential covariance functions are commonly used to model correlations decaying as a function of distance. \n",
    "Ignoring the time dimension, we would model the simultaneous temperatures throughout a region as a Gaussian process, with the covariance of two locations $\\xvec$ and $\\xvec'$\n",
    "\\begin{equation}\n",
    "    \\label{eq:kspace}\n",
    "    \\cov\\del{T(\\xvec), T(\\xvec') \\mid t} = k_{space}(\\xvec, \\xvec') = \\sigmaf^2 \\exp\\del{-\\frac{\\del{\\xvec-\\xvec'}\\trans\\del{\\xvec-\\xvec'}}{2\\ell_x^2}}\\,.\n",
    "\\end{equation}\n",
    "Similarly, ignoring the spatial dimension, the time series of temperatures at a single location can be modeled as a Gaussian process with covariance between two moments $t$ and $t'$\n",
    "\\begin{equation}\n",
    "\\cov\\del{T(t), T(t') \\mid \\xvec} = k_{time}(t, t') = \\sigmaf^2 \\exp\\del{-\\frac{\\del{t-t'}^2}{2\\ell_t^2}}\\,.\n",
    "\\end{equation}\n",
    "We then combine the spatial and temporal model by multiplying the covariance functions\n",
    "\\begin{equation}\n",
    "k_{st}(\\xvec,\\xvec',t,t') = k_{time}(t,t') \\cdot k_{space}(\\xvec, \\xvec')\\,.\n",
    "\\end{equation}\n",
    "This gives us the covariance of the Gaussian process underlying the full spatio-temporal model of temperatures.\n",
    "To complete the model specification, we add a mean temperature for each station $\\mu_{\\station{i}}$, and iid measurement noise $\\epsilon_i$.\n",
    "\\begin{align}\n",
    "    \\T_i &= \\mu_{\\station{i}} + f(\\xvec_i, t_i) + \\epsilon_i\\\\\n",
    "    f(\\xvec_i, t_i) &\\sim \\GP\\del{0, k_{st}(\\xvec,\\xvec',t,t')}\\\\\n",
    "    \\epsilon_i &\\overset{\\iid}{\\sim} \\normal\\del{0,\\sigman^2}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the spatiotemporal model\n",
    "\n",
    "Software is readily available in many programming languages for fitting Gaussian process models, including inference on the covariance parameters. We chose to use the julia `GaussianProcesses.jl` package to fit the above spatiotemporal model to the hourly temperatures at four Iowa weather stations. However, the Iowa data includes 47,864 measurements, which is computationally infeasible to fit directly with a single Gaussian process. While approximation techniques exist to fit such large datasets, we chose the less efficient but simpler approach of dividing the data into 10-day chunks, modeled as independent Gaussian processes with shared hyperparameters. To simplify our implementation, we replaced the $\\mu_\\station{i}$ terms by a spatial squared exponential component\n",
    "\\begin{equation}\n",
    "    k_{\\mu}(\\xvec, \\xvec') = \\sigma_\\mu^2 \\exp\\del{-\\frac{\\del{\\xvec-\\xvec'}\\trans\\del{\\xvec-\\xvec'}}{2\\ell_\\mu^2}}\n",
    "\\end{equation}\n",
    "with large variance $\\sigma_\\mu^2$ and low lengthscale $\\ell_\\mu$ added to the covariance function so that the spatio-temporal kernel becomes\n",
    "\\begin{equation}\n",
    "    k_{st}(\\xvec,\\xvec',t,t') = k_{time}(t,t') \\cdot k_{space}(\\xvec, \\xvec') + k_\\mu(\\xvec, \\xvec') \\,.\n",
    "\\end{equation}\n",
    "The model therefore has 4 free parameters: $\\sigmaf$, $\\ell_t$, $\\ell_x$ and $\\sigman$. We optimize the marginal likelihood of the Iowa data as a function of these three parameters\n",
    "\\begin{equation}\n",
    "\\label{eq:optimization}\n",
    "\\hat\\sigmaf,\\hat\\ell_t,\\hat\\ell_x,\\hat\\sigman = \\argmax_{\\sigmaf,\\ell_t,\\ell_x,\\sigman} \\cbr{ \\Pr\\del{ Y \\mid {\\sigmaf,\\ell_t,\\ell_x,\\sigman} } }\\,,\n",
    "\\end{equation}\n",
    "and obtain $\\hat\\sigmaf=3.73\\,\\degreeC$, $\\hat\\ell_t=2.7\\,\\mathrm{hours}$, $\\hat\\ell_x=176.4\\,\\mathrm{km}$ and $\\hat\\sigman=0.44\\,\\degreeC$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions using nearby data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:predict_nearby}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:predictive_nearby_SEonly}Predictive distribution using only nearby data and the simple product of square exponentials model. The orange lines are the measurements at nearby stations that are being used to inform the predictions. The black line is the true temperatures that have been withheld from the model, while the green line is the posterior mean of the predictions. The credible range (in green) is twice the standard deviations extracted from the diagonal entries of the posterior covariance matrix.](figures/predictive_nearby_SEonly.png)\n",
    "\n",
    "Once we have a spatio-temporal Gaussian process model with optimized covariance parameters, we can use it to generate predictions at the station where we aim to generate imputations based on nearby measurements.\n",
    "Gaussian processes make this a closed-form procedure. We'll denote the temperatures we wish to impute as $\\T_\\miss{}$ at times $t_\\miss$ and location $\\xvec_\\miss$ and those observed at nearby stations as $\\T_\\obs{}$, at times $t_\\obs$ and locations $X_\\obs$. \n",
    "Under the spatio-temporal model, $\\T_\\miss$ and $\\T_\\obs$ are jointly multivariate normal, with mean zero and covariance given by $k_{st}(\\xvec,\\xvec',t,t')$. \n",
    "Standard results for conditioning within multivariate normals then yields\n",
    "\\begin{equation}\n",
    "\\eqlabel{eq:unconstrained_post}\n",
    "\\begin{split}\n",
    "    \\T_\\miss \\mid \\T_\\obs &\\sim \\normal\\del{\\mu_{\\miss \\mid \\obs}, \\Sigma_{\\miss \\mid \\obs}}\\,, \\\\\n",
    "    \\mu_{\\miss \\mid \\obs} &= \\E \\del{\\T_\\miss \\mid \\T_\\obs} \\\\\n",
    "        &= \\cov\\del{\\T_\\miss, \\T_\\obs} \\cov\\del{\\T_\\obs, \\T_\\obs}^{-1} \\T_\\obs\\,, \\\\\n",
    "    \\Sigma_{\\miss \\mid \\obs} &= \\var \\del{\\T_\\miss \\mid \\T_\\obs} \\\\\n",
    "        &= \\cov\\del{\\T_\\miss,\\T_\\miss} - \\cov\\del{\\T_\\miss, \\T_\\obs} \\cov\\del{\\T_\\obs, \\T_\\obs}^{-1} \\cov\\del{\\T_\\obs, \\T_\\miss}\\,. \\\\ %_\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "All covariance matrices can be obtained by plugging into $k_{st}$. For example, the $ij$th entry of $\\cov\\del{\\T_\\miss, \\T_\\obs}$ is given by $k_{st}(\\xvec_\\miss,X_\\obs\\sbr{j},t_\\miss\\sbr{i},t_\\obs\\sbr{j})$, where $X_\\obs\\sbr{j}$ gives the spatial covariates of the $j$th observation, and $t_\\obs\\sbr{j}$ its time.\n",
    "\n",
    "In Figure \\ref{fig:predictive_nearby_SEonly}, we show an example of predictions obtained from this spatio-temporal model. We witheld measurements from the Waterloo Municipal Airport, and then used data from three nearby stations between May 25, 2015 and June 3, 2015 to predict the Waterloo temperatures during the same time window. This allows us to assess the quality of the predictions on this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputations\n",
    "\n",
    "## Imputing by Conditioning on Extrema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* introduce general problem\n",
    "* rejection sampling: doesn't work\n",
    "* MCMC approach: sample from a distribution\n",
    "    * spell out model\n",
    "    * sharp condition: add some tolerance\n",
    "    * still a difficult distribution to sample from\n",
    "        * 3D diagram of constraint (see Constrained distribution in OneNote)\n",
    "        * MCMC algorithm must navigate corners, and travel down edges efficiently\n",
    "    * HMC and Stan are particularly good at sampling from complicated distributions (citation?)\n",
    "* HMC uses the gradient to navigate complicated distributions\n",
    "    * but the maximum function is ill-behaved (expand)\n",
    "    * replace it with the smoothmax approximation (plot)\n",
    "* Demonstration\n",
    "    * without smoothmax and with smoothmax\n",
    "    * Stan code in appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim isn't simply to predict temperatures at a location with no measurements, but rather to impute hourly temperatures at a location with accurate measurements of the daily temperature extrema.\n",
    "This is an instance of a more general statistical problem: if a random $p$-vector $\\cbr{X_i:~i=1,\\ldots,p}$ has a known distribution $F_X$, and its maximum $\\Xmax \\equiv \\max_i\\cbr{X_i}$ and minimum $\\Xmax \\equiv \\max_i\\cbr{X_i}$ are measured, how does one draw samples from $\\Fcond$, the distribution of $X$ conditional on $\\Xmax$ and $\\Xmin$?\n",
    "Conditional draws from $\\Fcond$ need to respect three constraints: one component of $X$ must be equal to $\\Xmin$, another to $\\Xmax$, and all other components must lie between $\\Xmin$ and $\\Xmax$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, we could implement a valid imputation algorithm by drawing random samples $F_X$ \n",
    "(in our application, this is the posterior predictive multivariate normal distribution $\\T_\\miss \\mid \\T_\\obs$ obtained from nearby measurements),\n",
    "and only keeping the samples that satisfy the three constraints. \n",
    "Unfortunately, if $F_X$ is a continuous distribution, the probability of a random draw from $F_X$ exactly satisfying such sharp constraints is zero.\n",
    "One could envision adding some tolerance, so that samples with minimum and maximum within $\\epsilon$ of $\\Xmax$ and $\\Xmin$ are retained, but as $p$ grows, the rejection probability will rapidly go to 1, thus requiring huge sample sizes.\n",
    "Ultimately, this rejection sampling strategy is therefore bound to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chain Monte Carlo (MCMC) techniques can also be used to draw samples from arbitrary distributions with densities known up to a constant. The density of $\\Fcond$ is obtained up to a constant multiplier through a simple application of Bayes' theorem. It is proportional to the prior density of $F_X$ multiplied by indicators ensuring that the extrema are respected.\n",
    "\\begin{equation}\\begin{split}\n",
    "    \\Pr\\del{X \\mid \\Xmax,\\Xmin} &\\propto \\Pr\\del{X} \\Pr\\del{\\Xmax,\\Xmin \\mid X} \\,, \\\\\n",
    "           &\\propto \\Pr\\del{X} \\Ind\\del{ \\max_i\\cbr{X_i} = \\Xmax }\\Ind\\del{ \\min_i\\cbr{X_i} = \\Xmin } \\,.\n",
    "\\end{split}\\end{equation}\n",
    "However, this distribution is zero everywhere in $\\mathbb{R}^p$, except in a (p-2) dimensional subspace where the $\\min$ and $\\max$ constraints are met. \n",
    "This doomed the rejection sampler, and will also prevent any unmodifed MCMC algorithm from converging onto $\\Fcond$.\n",
    "We therefore approximate the constraint by replacing the likelihood term $\\Pr\\del{\\Xmax,\\Xmin \\mid X}$ with two narrow independent normal distributions around the minimum and maximum of $X$. \n",
    "This “softens” the conditional distribution,\n",
    "\\begin{equation}\n",
    "\\eqlabel{eq:normal_lik}\n",
    "\\begin{split}\n",
    "    \\Pr\\del{X \\mid \\Xmax,\\Xmin} &\\propto \\Pr\\del{X} \n",
    "                                         \\normal\\del{\\Xmax \\mid \\max_i\\cbr{X_i}, \\epsilon^2}\n",
    "                                         \\normal\\del{\\Xmin \\mid \\min_i\\cbr{X_i}, \\epsilon^2}\\,,\n",
    "\\end{split}\\end{equation}\n",
    "where $\\normal\\del{x \\mid \\mu, \\sigma^2}$ is the density of a normal distribution with mean $\\mu$ and variance $\\sigma^2$ evaluated at $x$. \n",
    "For small $\\epsilon$, this is a very tolerable approximation which enables the use of MCMC techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approximation to $\\Fcond$ remains a difficult distribution to sample from. \n",
    "We illustrate the constraint in a 3-dimensional setting in Figure \\ref{fig:constraints3d}.\n",
    "The MCMC algorithm must travel efficiently along the edges of the allowed subspace,\n",
    "and navigate corners when the index of the extremum components change.\n",
    "Hamiltonian Monte Carlo (HMC) has shown a a remarkable ability to navigate complicated distributions, including distributions where the typical set has “pinch points” of strong\n",
    "curvature <cite data-cite=\"betancourt2017conceptual\">(Betancourt, 2017)</cite>, similar to the “corners“ in $\\Fcond$. \n",
    "We therefore used HMC as implemented by the Stan probabilistic programming language <cite data-cite=\"stancite\">(Carpenter, 2017)</cite> to obtain draws from $\\Fcond$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMC's efficient sampling relies on gradient information in order to move towards regions of high probability.\n",
    "The normal likelihood \\eqref{eq:normal_lik} softened the extrema constraints,\n",
    "but the maximum and minimum functions also remove information from the gradient.\n",
    "The partial derivative of the log-likelihood of the maximum term with respect to $X_i$ is proportional to\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\log \\normal\\del{\\Xmax \\mid \\max_i\\cbr{X_i}, \\epsilon^2}}{\\partial X_i} \\propto \\del{\\Xmax - X_i} \\Ind\\cbr{\\argmax_j\\del{X_j} = i} \\,,\n",
    "\\end{equation}\n",
    "where $\\argmax$ is the function that returns the index of the maximum component.\n",
    "In other words, the gradient pulls the maximum of the current sample towards $\\Xmax$,\n",
    "and ignores all other components.\n",
    "This makes it difficult for HMC to efficiently explore scenarios where other components are the maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assist the HMC algorithm with another approximation.\n",
    "We replace the $\\max$ and $\\min$ functions with the $\\softmax$ and $\\softmin$ functions, which take real inputs $x_1, \\ldots, x_p$ and a sharpness parameter $k$ and return\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\softmax\\del{x_1, \\ldots, x_p ; k} &= \\frac{1}{k} \\log\\del{\\sum_{i=1}^p e^{kx_i}} \\\\\n",
    "    \\softmin\\del{x_1, \\ldots, x_p ; k} &= -\\softmax\\del{-x_1, \\ldots, -x_p; k}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "As $k \\rightarrow \\infty$, $\\softmax$ becomes the maximum, and $\\softmin$ becomes the minimum. When $\\softmax$ replaces $\\max$ and $\\softmin$ replaces $\\min$, there is a small price in precision due to the approximation, but there is a huge computational benefit: the gradient is now informative for all components of $X$:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\log \\normal\\del{\\Xmax \\mid \\softmax\\del{X_{1:p} ; k}, \\epsilon^2}}{\\partial X_i} \\propto \\del{\\Xmax - \\softmax\\del{X_{1:p} ; k}} \n",
    "        \\frac{e^{k X_i}}\n",
    "             {\\sum_{j=1}^p e^{k X_j}} \\,.\n",
    "\\end{equation}\n",
    "These modifications make HMC a viable algorithm to efficiently draw samples from the constrained posterior. \n",
    "Setting $k$ and $\\sigma_\\epsilon$ is a compromise between exactness and efficiency; \n",
    "we found $k=10$ and $\\sigma_\\epsilon=0.1$ to perform well for this paper's application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of Hamiltonian Monte Carlo with Smoothmax Approximation\n",
    "\n",
    "We verify the ability of the Hamiltonian Monte Carlo algorithm with the smoothmax approximation to obtain draws from $\\Fcond$ in a simplified setting where the distribution function of $\\Fcond$ can be derived analytically and also computed easily.\n",
    "In our application, $F_X$ is the posterior predictive multivariate normal distribution $\\T_\\miss \\mid \\T_\\obs$ obtained from nearby measurements, with mean and marginal variance evolving smoothly from one prediction to the next.\n",
    "To retain a resemblence to this, we specify a random vector $X$ with each component $X_i$ normally distributed, and with sinusoidal means and variances, but without any correlations between them,\n",
    "so as to avoid a combinatorial explosion when obtaining the distribution function analytically:\n",
    "\\begin{equation}\n",
    "\\eqlabel{eq:toyspec}\n",
    "\\begin{split}\n",
    "X_i &\\sim \\normal \\del{\\mu_i, \\sigma_i} \\,, \\\\\n",
    "X_i & \\indep X_j ~\\forall\\, i \\neq j \\,, \\\\\n",
    "\\Xmax &= \\max_i\\cbr{X_i} \\,, \\\\\n",
    "\\Xmin &= \\min_i\\cbr{X_i} \\,,\\\\\n",
    "\\mu_i &= 10 + \\sin\\del{2\\pi i / 50} \\,, \\\\\n",
    "\\sigma_i &= 0.1+\\cos^2\\del{2\\pi i / 50} \\,, \\\\\n",
    "i &= 1, 2, \\ldots, 100 \\,.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "The unconstrained distribution of $X_i$ is illustrated in Figure \\ref{fig:toy_quantiles}(a).\n",
    "In this example, we aim to sample from the distribution of $X_i$ subject to the observation that $\\Xmax=12.5$ and $\\Xmin=8.8$.\n",
    "\n",
    "We first derive and compute $\\Fcond$ for this example.\n",
    "We denote by $f_i(\\cdot)$ and $F_i(\\cdot)$ the prior probability distribution function and cumulative distribution function of $X_i$, i.e. the normal PDF and CDF with means and variances given by \\eqref{eq:toyspec}.\n",
    "Let $\\pij$ be the probability that $X_i$ is the minimum of $X$, and $X_j$ is its maximum.\n",
    "We also define $\\pisum = \\sum_{j=1}^{100} \\pij$, the probability that $X_i$ is the minimum,\n",
    "and $\\psumj = \\sum_{i=1}^{100} \\pij$, the probability that $X_j$ is the maximum.\n",
    "The cumulative distribution function of $X_i$ is then given by\n",
    "\\begin{equation}\n",
    "\\Pr\\del{X_i \\leq x \\mid \\Xmax, \\Xmin} =\n",
    "    \\begin{cases}\n",
    "        0 &\\text{when } x < \\Xmin \\,, \\\\\n",
    "        \\pxx{i}{\\bullet} \n",
    "            + \\del{1 - \\pxx{i}{\\bullet} - \\pxx{\\bullet}{i}}\n",
    "            \\sbr{\\frac{F_i(x) - F_i(\\Xmin) }\n",
    "                 {F_i(\\Xmax) - F_i(\\Xmin) }\n",
    "                } \n",
    "            &\\text{when } \\Xmin \\leq x < \\Xmax \\,, \\\\\n",
    "        1 &\\text{when } x \\geq \\Xmax \\,.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "Meanwhile, $\\pij$ is proportional to\n",
    "\\begin{equation}\n",
    "    f_i(\\Xmin)\n",
    "    f_j(\\Xmax)\n",
    "    \\prod_{k \\neq i,j}^{100}\n",
    "    \\del{F_k(\\Xmax) - F_k(\\Xmin)} \\,,\n",
    "\\end{equation}\n",
    "which we compute for all $i,j$ and renormalize\n",
    "to obtain the $100 \\times 100$ matrix of probabilities.\n",
    "We sum over its rows and columns to obtain $\\psumj$ and $\\pisum$.\n",
    "While this algorithm has cubic complexity in the dimensionality $p$ of $X$,\n",
    "for $p=100$ \n",
    "computers only take seconds to compute the entries of $\\Pr$ and evaluate $\\Pr\\del{X_i \\leq x \\mid \\Xmax, \\Xmin}$ over a range of $x$.\n",
    "Figure \\ref{fig:toy_quantiles}(b) shows the analytical quantiles of $\\Fcond$.\n",
    "Roughly speaking, we see that the prior distribution $F_X$ is stretched to fit between $\\Xmin$ and $\\Xmax$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:toy_quantiles}(a) Prior distribution of $X_i$ displayed as mean function with $2\\sigma$ envelope; (b) Quantiles of the analytically derived posterior $\\Fcond$ conditioned on $\\Xmin$ and $\\Xmax$, with prior $\\mu_i$ shown in blue; (c) Quantiles of the samples drawn from $\\Fcond$ using Stan without the $\\softmax$ approximation, with prior $\\mu_i$ shown in blue; (d) Quantiles of the samples drawn from $\\Fcond$ using Stan with the $\\softmax$ approximation, with prior $\\mu_i$ shown in blue.](figures/toy_quantiles.png)\n",
    "\n",
    "To obtain samples from $\\Fcond$, we use the implementation of Hamiltonian Monte Carlo provided by the probabilistic programming language Stan.\n",
    "In Stan, the user specifies a probabilistic data-generating process for the observed data, based on parameters and latent variables with accompanying priors.\n",
    "Stan then compiles this model into a custom `C++` program that implements\n",
    "posterior sampling using HMC.\n",
    "We implement two Stan models to draw from $\\Fcond$.\n",
    "The Stan model code for both are available in appendix \\ref{sec:stan_illustration}.\n",
    "The first model implements equation \\eqref{eq:normal_lik}, \n",
    "with the narrow normal likelihood term around the maximum and minimum, \n",
    "while the second model includes the $\\softmax$ and $\\softmin$ approximations to the maximum and minimum functions.\n",
    "For each Stan model, we obtain 4 HMC chains each with 10,000 warm-up samples followed by 10,000 samples.\n",
    "The quantiles of the samples obtained without the $\\softmax$ approximation are shown in Figure \\ref{fig:toy_quantiles}(c).\n",
    "By default, Stan initizializes each $X_i$ uniformly at random between -2 and 2,\n",
    "and for most variables, the algorithm remains stuck near the initial values.\n",
    "Furthermore, most samples do not conform to the constraints imposed by the observed $\\Xmin$ and $\\Xmax$ values, which further invalidates these results.\n",
    "However, once we replace the maximum function with the $\\softmax$ function,\n",
    "with quantiles shown in Figure \\ref{fig:toy_quantiles}(d),\n",
    "Stan is able to draw samples that respect the observed extrema.\n",
    "Furthermore, a visual comparison of the analytical quantiles in Figure \\ref{fig:toy_quantiles}(a)\n",
    "and the Stan sample quantiles in Figure \\ref{fig:toy_quantiles}(d) confirms that \n",
    "this sampling algorithm delivers a close approximation of the marginal distribution of each variable $X_i$ in $\\Fcond$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:toy_joint}\n",
    "Comparison of the joint distribution of $X_{23}$ and $X_{52}$\n",
    "obtained analytically and from Stan samples.\n",
    "](figures/toy_joint.png)\n",
    "\n",
    "We may also wish to verify that Stan samples correctly from the *joint* distribution of any combination of variables. \n",
    "We do this visually for a pair of variables, $X_{23}$ and $X_{52}$, with results shown in Figure \\ref{fig:toy_joint}.\n",
    "In that figure, the central scatterplot shows the 40,000 Stan samples obtained using the $\\softmax$ approximation.\n",
    "Superimposed on the scatterplot are a contour plot (with dash-dotted lines) of the probability distribution function of the analytical conditional distribution $\\Fcond$ of $X_{23}$ and $X_{52}$ when neither $X_{23}$ nor $X_{52}$ is one of the extrema, multiplied by the probability of that being the case (which is $1 - \\del{\\pxx{23}{\\bullet} + \\pxx{52}{\\bullet} + \\pxx{\\bullet}{23} + \\pxx{\\bullet}{52}} + \\del{\\pxx{23,}{52} + \\pxx{23,}{52}}$).\n",
    "This can be compared to the contour plot (solid lines) of the same probability distribution function obtained through a kernel density estimator of the subset of Stan samples where neither $X_{23}$ and $X_{52}$ is the minimum or maximum,\n",
    "using a normal kernel with bandwidth $0.2$,\n",
    "and multiplied by the proportion of samples where that is the case.\n",
    "The kernel estimates are divided by the integrated probability mass of the kernel that is inside of the boundaries, in order to reduce boundary effects.\n",
    "The thin black dotted line are one kernel bandwidth away from the $\\Xmin$/$\\Xmax$ boundaries.\n",
    "Outside the thin black dotted line, the kernel density estimates are less trustworthy.\n",
    "The four histograms around the scatter plot are of the Stan samples where one of the variables is an extremum, weighted so as to integrate to the fraction of samples that satisfy that condition.\n",
    "For example, the top histogram is of $X_{23}$ for samples where $X_{52}$ is the maximum,\n",
    "and integrates to the fraction of samples where that is the case.\n",
    "The super-imposed pink line is that of a truncated normal probability distribution function\n",
    "multiplied by the probability of the satisfied condition.\n",
    "For example, the pink line over the top histogram integrates to $\\pxx{\\bullet}{52}$.\n",
    "Lastly, the blue and red lines are $\\Xmin$ and $\\Xmax$ respectively.\n",
    "There is a close match between the contours of the analytical joint distribution function (dash-dotted lines) and of the kernel density estimate of the Stan samples.\n",
    "Each of the four histogram of samples where $X_{23}$ or $X_{52}$ occupies the minimum or maximum position matches the corresponding analytical distribution function.\n",
    "This visual comparison of the sample and analytical distributions should reassure us that Stan is yielding a good approximation of a sample drawn from the true $\\Fcond$ in this example.\n",
    "We did not examine the behavior of the sampling algorithm for the joint distribution of more than two variables due to the difficulty of visualizing such a distribution,\n",
    "but we see no reason to suspect that the algorithm suffers from pathological behavior that does not manifest itself in these univariate and bivariate inspections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothmax Temperature Model\n",
    "\n",
    "![\\label{fig:imputations_SEonly}Imputations at Waterloo Airport from May 25, 2015 to June 3, 2015](figures/imputations_SEonly.png)\n",
    "\n",
    "Armed with the $\\softmax$ approximation implemented in Stan, we finally return to the problem of imputing hourly temperature measurements.\n",
    "A small leap of faith is needed to accept that the success of the strategy that we implemented and tested in a toy example in the previous two sections will extend to this application.\n",
    "There a three important differences between the toy example and the temperature model.\n",
    "Firstly, $\\Fcond$ is now a multivariate normal distribution with strong correlations obtained as the posterior of a Gaussian process in section.\n",
    "Secondly, instead of a single minimum and maximum, we observe extrema for every 24 hour period.\n",
    "Thirdly, we allow for the mean temperature to be different at different locations,\n",
    "and so the imputed temperatures are shifted by an additional parameter $\\mu_{\\miss}$,\n",
    "to which we will attach a vague prior.\n",
    "To summarize, the probabilistic model that we wish to draw posterior imputations of $\\T_\\miss$ from is given in \\eqref{eq:idealmodel}.\n",
    "\\begin{equation}\n",
    "\\eqlabel{eq:idealmodel}\n",
    "\\begin{aligned}\n",
    "    \\mu_{\\miss} &\\sim \\normal\\del{0,100} & \\text{ (vague prior on mean temperature)} \\\\\n",
    "    f_\\miss &\\sim \\normal\\del{\\mu_{\\miss \\mid \\obs}, \\Sigma_{\\miss \\mid \\obs}} & \\text{ (posterior from $\\T_\\obs$ becomes prior)} \\\\\n",
    "    \\T_\\miss &= \\mu_\\miss + f_\\miss \\\\\n",
    "    \\Tx\\sbr{\\iday} &= \\max_{i \\in \\dayset{\\iday}}\\cbr{ \\T_{\\miss,i}} & \\text{ (observe maximum in 24hr window)}\\\\\n",
    "    \\Tn\\sbr{\\iday} &= \\min_{i \\in \\dayset{\\iday}}\\cbr{ \\T_{\\miss,i}} & \\text{ (observe minimum in 24hr window)}\\\\\n",
    "    \\dayset{\\iday} &= \\cbr{i : \\iday-1+\\frac{\\hour}{24} \\lt t_{\\miss,i} \\le \\iday + \\frac{\\hour}{24}} & \\text{ (indices of times in the 24hr window)}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from this model, we modify it with the $\\softmax$ approximation to the maximum, and a normal likelihood. The resulting model is shown in \\eqref{eq:smoothed_model}, and the corresponding Stan code is in Appendix \\ref{sec:appendix_stan}.\n",
    "\\begin{equation}\n",
    "\\eqlabel{eq:smoothed_model}\n",
    "\\begin{split}\n",
    "    \\mu_\\miss &\\sim \\normal\\del{0,100} \\\\\n",
    "    f_\\miss &\\sim \\normal\\del{\\mu_{\\miss \\mid \\obs}, \\Sigma_{\\miss \\mid \\obs}} \\\\\n",
    "    T_\\miss &= \\mu_\\miss + f_\\miss \\\\\n",
    "    \\Tx\\sbr{\\iday} &\\sim \\normal\\del{\\softmax_{i \\in \\dayset{\\iday}} \\cbr{ T_{\\miss,i}; k=10}, 0.1^2} \\\\\n",
    "    \\Tn\\sbr{\\iday} &\\sim \\normal\\del{\\softmin_{i \\in \\dayset{\\iday}} \\cbr{ T_{\\miss,i}; k=10}, 0.1^2}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\n",
    "    \\label{fig:imputations_SEonly}\n",
    "    Imputations obtained using a product of squared exponential kernels.\n",
    "    ](\n",
    "    figures/imputations_SEonly.png)\n",
    "\n",
    "Example imputations from this procedure are shown in Figure \\ref{fig:imputations_SEonly}. \n",
    "From May 25, 2015 to June 3, 2015, hourly temperatures are imputed at Waterloo Airport, using the hourly temperature measurements from nearby stations to inform the course of the temperatures, and using the daily minima and maxima “measurements” to constrain the imputed temperatures, and to infer the mean. \n",
    "Because we actually have hourly data for Waterloo, yet only fed our algorithm a reduction of this data to daily extremes, we can also plot the hidden temperatures, and see how faithfully the imputations reproduce them.\n",
    "We see that the imputations indeed track the true measurements very closely. \n",
    "The error bars satisfyingly narrow and widen in accordance to the amount of information available at each moments. \n",
    "On May 27th, we can see that the imputations capture the fact that the $\\Tx$ record *could* have been set early in the measurement window, but more likely at its very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model diagnostics\n",
    "\n",
    "## Variogram\n",
    "\n",
    "We can visually inspect our model by plotting temporal and spatial semi-variograms. The semi-variogram of a stationary spatio-temporal function $Y(\\xvec,t)$ is a function of the spatial lag $\\hvec$ and the temporal lag $r$\n",
    "\\begin{equation}\n",
    "    \\gamma\\del{\\hvec,r} = \\frac{1}{2} \\E\\sbr{\\del{Y\\del{\\xvec,t}-Y\\del{\\xvec+\\hvec,t+r}}^2} = \\var\\del{ Y\\del{\\xvec,t}} - \\cov\\del{ \\del{Y\\del{\\xvec,t}} , Y\\del{\\xvec+\\hvec,t+r}}\\,.\n",
    "\\end{equation}\n",
    "For a Gaussian Process model, with a stationary kernel $k(\\hvec,r)=k(\\xvec,\\xvec+\\hvec,t,t+r)$ this can be expressed in terms of the observation noise $\\sigman$ and kernel function $k(\\cdot,\\cdot)$, as\n",
    "\\begin{equation}\n",
    "    \\label{eq:gp_variogram}\n",
    "    \\gamma\\del{\\hvec,r} = \\sigman^2 + k\\del{0,0} - k(\\hvec,r)\\,.\n",
    "\\end{equation}\n",
    "From the data, the semi-variogram can also be estimated empirically, be averaging the square differences of any two observations that are separated by $\\hvec$ in space, and $r$ in time (or, in practice, within half a bin width of $\\hvec$ and $r$). By comparing the empirical variogram to the variogram of our fitted $\\GP$ model, we obtain a visual diagnosis of the model.\n",
    "\n",
    "In our Iowa example, there are only four possible locations. For each location, we plot the empirical temporal variogram $\\hat\\gamma\\del{0,r}$. For any pair of stations separated by $\\hvec$ (fixed), we can also plot $\\hat\\gamma\\del{\\hvec,r}$. We then overlay the model's semi-variogram obtained through equation \\eqref{eq:gp_variogram}, resulting in Figure \\ref{fig:spatial_variogram}.\n",
    "\n",
    "![\\label{fig:spatial_variogram} Semi-variogram](figures/spatial_variogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the variogram of the simple `SExSE` model tracks the empirical variogram well at short lags, but fails to capture the diurnal cycle, and the fit degrades at long lag. We attempt to improve the model in section \\ref{sec:improving_model}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and expected error\n",
    "\n",
    "The variogram gives us a visual diagnostic of the overall model fit. To quantify the model's predictive ability in the Iowa example, we compare the posterior mean temperature to the withheld truth, and obtain the empirical mean squared error as\n",
    "\\begin{equation}\n",
    "    \\label{eq:mse}\n",
    "    \\mse\\del{\\error \\mid \\T_\\obs,\\Tx,\\Tn} = \\frac{1}{N} \\sum_{i=1}^N \\sbr{\\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs,\\Tx,\\Tn} - \\T_{\\miss,i}}^2\\,.\n",
    "\\end{equation}\n",
    "This equation is for the final predictions obtained using nearby hourly temperatures and local daily maxima and minima. \n",
    "A similar diagnostic can be computed for the intermediary predictions, which exclude the local $\\Tx$ and $\\Tn$ information. \n",
    "At that stage, we are not concerned with any overall bias in the predicted temperatures, so we instead compute the sample variance of the errors as\n",
    "\\begin{equation}\n",
    "    \\label{eq:varerr}\n",
    "    \\var\\del{\\error \\mid \\T_\\obs} = \\var_i \\cbr{\\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs} - \\T_{\\miss,i}}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "| Model                           | Log Likelihood | Var(err) | $\\E$(Var(err)) | MSE(err) | $\\E$(MSE(err)) |\n",
    "|---------------------------------|----------------|----------|----------------|----------|----------------|\n",
    "| SE x SE                         | -55,614        | 1.589    | 0.875          | 1.104    | 0.614          |\n",
    "| SExSE + diurnal                 | -54,472        | 1.633    | 0.974          | 1.137    | 0.697          |\n",
    "| Sum of products, fixed variance | -48,589        | 4.991    | 8.791          |          |                |\n",
    "| SoP, fixed temporal, free var   | -47,082        | 1.314    | 2.321          | 1.150    | 0.897          |\n",
    "| SoP, completely free            | -46,184        | 1.423    | 1.765          | 1.152    | 0.950          |\n",
    "| SoP, simpler                    | -45,945        | 1.319    | 1.190          | 1.069    | 0.823          |\n",
    "\n",
    "For our purposes, it isn't sufficient for the spatio-temporal model to yield good predictions; we also require a good estimate of its own accuracy. \n",
    "We estimate the expected MSE and predictive variance by sampling $K$ random draws $\\T^k_\\miss$ from the posterior distribution, again conditioned firstly on just $\\T_\\obs$ after fitting the spatio-temporal Gaussian process model, and then additionally on $\\T_\\obs$, $\\Tx$ and $\\Tn$ after incorporating the local data using Stan.\n",
    "The draws are obtained from the posterior multivariate normal distribution in the first case, and the MCMC samples obtained through Stan in the second case.\n",
    "We then evaluate the variance or MSE between the samples and the posterior mean as\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\E \\del{\\var\\del{\\error \\mid \\T_\\obs}} &\\approx \\frac{1}{K} \\sum_{k=1}^K \\var_i \\cbr{\\T^{(k)}_{\\miss,i} - \\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs}} \\\\\n",
    "    \\E \\del{\\mse\\del{\\error \\mid \\T_\\obs,\\Tx,\\Tn}} &\\approx \\frac{1}{K} \\sum_{k=1}^K \\mse_i \\cbr{\\T^{(k)}_{\\miss,i} - \\E\\del{\\T_{\\miss,i} \\mid \\T_\\obs,\\Tx,\\Tn}} \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "When evaluating models, we want the errors to be small, and so the empirical error variance and MSE to be low. A well-calibrated model should also have the expected error variances $\\E \\del{\\var\\del{\\error \\mid \\cdot}}$ close to their empirical values.\n",
    "\n",
    "These diagnostics for our first spatio-temporal model, the product of squared exponentials, are found in the first row of Table X. The empirical error variance using only nearby measurements is already fairly low, with typical errors of order $\\sqrt{1.589}=1.26\\,\\degreeC$. Incorporating the local measurements reduces it further to $\\sqrt{1.104}=1.05\\,\\degreeC$. However, the model is overly optimistic, and the expected errors underestimate the true errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:improving_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we develop more sophisticated Gaussian process models than the simple product of squared exponential kernels. We then assess whether these models improve the variogram and the predictive diagnostic measures that we developed in the previous sections.\n",
    "\n",
    "The most salient feature of the empirical variogram that isn't captured by the `SExSE` model is the oscillation with a 24-hour period. It is intuitively obvious that the diurnal cycle induces this periodic covariance, and that our model should be improved by incorporating this feature. Gaussian process models allow for periodic components of the covariance, for example the periodic squared exponential kernel, which we will use with a 24-hour period\n",
    "\\begin{equation}\n",
    "    k_{24}(t,t') = \\sigma_{24}^2 \\exp\\sbr{ - \\frac{2}{\\ell_{24}^2} \\sin^2\\del{\n",
    "        \\pi \\frac{t-t'}{\\text{24 hrs}} \n",
    "        }}\\,.\n",
    "\\end{equation}\n",
    "We modify the spatiotemporal model by adding this diurnal component to it, with its own spatial decay kernel $k_{space24}$ (with the same specification as $k_{space}$ in \\eqref{eq:kspace}).\n",
    "\\begin{equation}\n",
    "    \\kdiurn(\\xvec,\\xvec',t,t') = k_{time}(t,t') \\cdot k_{space}(\\xvec, \\xvec') \n",
    "        + k_{24}(t,t') \\cdot k_{space24}(\\xvec, \\xvec')\n",
    "        + k_\\mu(\\xvec, \\xvec') \n",
    "        \\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also develop a more complex model, which breaks up $k_{time}$ into short-term, medium-term and long-term correlation components, each with their own spatial decay.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    \\ksumprod(\\xvec,\\xvec',t,t') &= \n",
    "           k_{time1}(t,t') \\cdot k_{space1}(\\xvec, \\xvec')  &\\text{(short-term variation)} \\\\\n",
    "        &+ k_{time2}(t,t') \\cdot k_{space2}(\\xvec, \\xvec')  &\\text{(medium-term variation)} \\\\\n",
    "        &+ k_{time3}(t,t') \\cdot k_{space3}(\\xvec, \\xvec')  &\\text{(long-term variation)} \\\\\n",
    "        &+ k_{24}(t,t') \\cdot k_{space24}(\\xvec, \\xvec') &\\text{(diurnal cycle)} \\\\\n",
    "        &+ k_\\mu(\\xvec, \\xvec') &\\text{(station mean)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Each of $k_{time1}$, $k_{time2}$, and $k_{time3}$, is a rational quadratic kernel\n",
    "\\begin{equation}\n",
    "    k_{RQ}(t,t') = \\sigma^2 \\del{1 + \\frac{\\del{t-t'}^2}{2\\alpha\\ell^2} }^{-\\alpha}\n",
    "\\end{equation}\n",
    "which is accompanied by its spatial decay kernel, specified as a squared exponential covariance.\n",
    "This more complicated kernel therefore has $3 \\times 3 \\times 2 + 2 \\times 2 = 22$ free parameters, in addition to the noise parameter $\\sigman^2$.\n",
    "\n",
    "We now have three competing Gaussian process models, with covariance functions $\\kSESE$, $\\kdiurn$, and $\\ksumprod$ respectively. We can compare them in three ways. Firstly, the marginal log-likelihood is the quantity maximized by the parameter fitting procedure in \\eqref{eq:optimization}. The maximized log-likelihood can be found in the second column of Table XX, and we see that the more complex models indeed yield a much higher log-likelihood, promising a better model fit which should yield better predictions. Secondly, we compare the variance of the error in the predicted temperatures specified in \\eqref{eq:varerr} when withholding all the data from a test station. Averaged over all of 2015, this is given in the third column, and shows more mixed results. The diurnal model $\\kdiurn$ performs worse than the simple $\\kSESE$ model, and $\\ksumprod$ only yields a small improvement. \n",
    "Thirdly, we can reintroduce the daily minima and maxima from the withheld station, and compare the mean squared error specified in \\eqref{eq:mse} for predictions at the test station. Results in the fifth column show even more modest improvements for the more complex models.\n",
    "\n",
    "We interpret these results as a reminder that predictions using Gaussian process are sensitive to model specification when extrapolating, but fairly insensitive to the model when interpolating [cite?]. Since our imputations interpolate the temperatures from nearby stations, further aided by the constraints imposed by the daily $\\Tn$ and $\\Tx$ measurements, the choice of model does not have a large impact on the performance of our procedure. This insensitivity can be seen as reassuring, as it (to an extent) reduces our need to worry about the incorrectness of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "* show imputations on interesting days\n",
    "* show imputations can capture two possible explanations for a measurement\n",
    "* discuss possibility of inferring measurement time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on measurement hour\n",
    "\n",
    "Our analysis so far has focused on the case where the hour of measurement $\\hour$ is known in advance.\n",
    "This is an unrealistic assumption in practice, and so inference on $\\hour$ is a desirable feature.\n",
    "It is conceptually straightforward to modify \\eqref{eq:smoothed_model} with a uniform prior on $\\hour$.\n",
    "However, because we obtain our imputations in ten-day windows, in most windows precise information about $\\hour$ will not be available, as moving the measurement time one hour earlier or later rarely affects the measured $\\Tn$ and $\\Tx$.\n",
    "Furthermore, $\\hour$ affects which observations are attributed to each day's measurements. \n",
    "This effect is discontinuous (observations suddenly jump from one day to the next) and non-differentiable, and so Hamiltonian Monte Carlo becomes unviable.\n",
    "This issue is similar to that caused by the non-differentiability of the minimum and maximum functions.\n",
    "We therefore do not consider the introduction of a uniform prior on $\\hour$ in Stan to be feasible.\n",
    "\n",
    "Our procedure allows us to obtain imputation samples of $\\T_\\miss$ conditional on $\\T_\\obs,\\Tn,\\Tx$ and $\\hour$. \n",
    "If we do so for $\\hour=1,2,\\ldots,24$, is there information available in these samples to infer $\\hour$?\n",
    "We will examine sample imputations to answer this question.\n",
    "Figure \\ref{fig:measure_hour_example} shows mean imputation for temperatures over nine days starting on February 27, 2015. The orange line is the mean using only nearby temperatures (shifted by a constant to match the true temperatures), while the green line is additionally conditional on $\\Tn$ and $\\Tx$; the true temperatures are shown in grey.\n",
    "The top plot shows the imputation under the correct daily measurement time (17 UTC), while the bottom plot is under an incorrect measurement time (5 UTC).\n",
    "The first unsurprising observation is that assuming an incorrect measurement time can lead to wildly inaccurate imputations.\n",
    "But we then also notice that assuming the wrong time also causes the mean constrained imputation to depart further from the unconstrained imputation \n",
    "(that is, the green and orange lines are further apart).\n",
    "This can be interpreted as an indication of an incompatibility between $\\T_\\obs$ and the daily extremes, caused by assuming the wrong $\\hour$.\n",
    "To quantify this discrepancy, we propose to calculate the probability of the mean constrained imputation under the unconstrained posterior given by $\\eqref{eq:unconstrained_post}$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\mu\\del{\\hour} &\\equiv \\E\\del{ \\T_\\miss \\mid \\T_\\obs, \\Tn, \\Tx, \\hour } \\text{ (the mean imputed temperature), }\\\\\n",
    "    \\discrepancy_\\hour &\\equiv \\Pr\\del{ \\T_\\miss = \\mu\\del{\\hour} \\mid \\T_\\obs }\\,.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Our intuition is that $\\discrepancy_\\hour$ will drop sharply when the wrong $\\hour$ is assumed,\n",
    "and we may be able to infer the true $\\hour$ by maximizing $\\discrepancy_\\hour$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:measure_hour_example} A sample window showing constrained and unconstrained imputations assuming (top) the correct measurement hour (17 UTC), and (bottom) the wrong measurement hour (5 UTC). Assuming the wrong measurement time drives the constrained mean imputation away from the unconstrained mean imputation.](figures/measure_hour_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\\label{fig:hr_inference} Discrepancy measure for imputations of temperatures at Waterloo Municipal Airport assuming measurement hours $\\hour=1,2,\\ldots,24$. The true hour of measurement is 17, and obtains the highest $\\delta_\\hour$.](figures/hr_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortuitously, our discrepancy measure $\\discrepancy_\\hour$ also admits a Bayesian interpretation: is is proportional to the marginal likelihood of $\\hour$ under (admittedly fanciful) approximating assumptions.\n",
    "Ideally, we would evaluate the marginal likelihood $\\Pr\\del{\\Tn,\\Tx \\mid \\T_\\obs, \\hour}$, and then appeal to Bayes theorem to obtain a posterior on $\\hour$\n",
    "\\begin{equation}\n",
    "    \\Pr\\del{\\hour \\mid \\Tn, \\Tx, \\T_\\obs } \\propto \\Pr\\del{\\Tn,\\Tx \\mid \\T_\\obs, \\hour} \\Pr\\del{\\hour}\\,.\n",
    "\\end{equation}\n",
    "However, marginal likelihoods are notoriously difficult to estimate from posterior samples [cite? Raftery 1994?].\n",
    "The marginal likelihood is the normalizing constant for the posterior \\eqref{eq:posterior_constrained} of $\\T_\\miss$, and therefore for any $\\T_\\miss$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\Pr\\del{\\Tn,\\Tx \\mid \\T_\\obs, \\hour} &= \\frac{\n",
    "        \\Pr\\del{\\Tn,\\Tx \\mid \\T_\\miss, \\cancel{\\T_\\obs}, \\cancel{\\hour}} \n",
    "        \\Pr\\del{\\T_\\miss \\mid \\T_\\obs, \\cancel{\\hour}}\n",
    "    } { \n",
    "        \\Pr\\del{\\T_\\miss \\mid \\Tn,\\Tx,\\T_\\obs, \\hour}\n",
    "    }\\,.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "The first term in the numerator is either one or zero, as discussed near equation \\eqref{eq:constraint}.\n",
    "If we assume the constraint is satisfied,\n",
    "pick $\\T_\\miss = \\mu\\del{\\hour}$,\n",
    "and assume that the posterior density evaluated at its mean\n",
    "does not depend heavily on the time of measurement,\n",
    "we obtain that the marginal likelihood is proportional to $\\discrepancy_\\hour$.\n",
    "Both assumptions are fanciful: the posterior mean generally will violate the constraint imposed by $\\Tn$ and $\\Tx$, and therefore the likelihood $\\Pr\\del{\\Tn,\\Tx \\mid \\T_\\miss}$ should in fact be zero. \n",
    "Furthermore, there is no reason to think the posterior density at the posterior mean does not depend on $\\hour$, but we might reasonably hope that the wrongness of this assumption does not overwhelm the signal contained in $\\discrepancy_\\hour$.\n",
    "This reasoning at least confirms that $\\discrepancy_\\hour$ captures information about the likelihood of $\\hour$, and that once renormalized it can be loosely interpreted as a posterior probability under a uniform prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices\n",
    "\n",
    "## Stan programs for illustration of smoothmax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:stan_illustration}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without `smoothmax` Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```stan   \n",
    "data {\n",
    "    int<lower=0> N; // number of observations\n",
    "    real Xmax;\n",
    "    real Xmin;\n",
    "    vector[N] mu_i;\n",
    "    real<lower=0> sigma_i[N];\n",
    "}\n",
    "parameters {\n",
    "    vector[N] X_i; // latent variables\n",
    "}\n",
    "model {\n",
    "    X_i ~ normal(mu_i, sigma_i);\n",
    "    Xmax ~ normal(max(X_i), 0.01);\n",
    "    Xmin ~ normal(min(X_i), 0.01);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `smoothmax` Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```stan\n",
    "functions {\n",
    "    real smoothmax(vector x, real k, real maxkx){\n",
    "        return (maxkx+log(sum(exp(k*x - maxkx))))/k;\n",
    "    }\n",
    "    real smoothmin(vector x, real k, real minkx){\n",
    "        return -smoothmax(-x, k, -minkx);\n",
    "    }\n",
    "}\n",
    "data {\n",
    "    int<lower=0> N; // number of observations\n",
    "    real Xmax;\n",
    "    real Xmin;\n",
    "    real mu_i[N];\n",
    "    real<lower=0> sigma_i[N];\n",
    "    real<lower=0> k;\n",
    "}\n",
    "parameters {\n",
    "    vector[N] X_i; // latent variables\n",
    "}\n",
    "transformed parameters {\n",
    "    real Xsmoothmax;\n",
    "    real Xsmoothmin;\n",
    "    Xsmoothmax = smoothmax(X_i, k, k*Xmax);\n",
    "    Xsmoothmin = smoothmin(X_i, k, k*Xmin);\n",
    "}\n",
    "model {\n",
    "    X_i ~ normal(mu_i, sigma_i);\n",
    "    Xmax ~ normal(Xsmoothmax, 0.01);\n",
    "    Xmin ~ normal(Xsmoothmin, 0.01);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stan model for temperature imputations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\label{sec:appendix_stan}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "functions {\n",
    "    real smoothmax(vector x, real k, real maxkx){\n",
    "        return (maxkx+log(sum(exp(k*x - maxkx))))/k;\n",
    "    }\n",
    "    real smoothmin(vector x, real k, real minkx){\n",
    "        return -smoothmax(-x, k, -minkx);\n",
    "    }\n",
    "}\n",
    "data {\n",
    "    // Tn Tx data\n",
    "    int<lower=1> N_TxTn; //\n",
    "    vector[N_TxTn] Tx;\n",
    "    vector[N_TxTn] Tn;\n",
    "\n",
    "    // imputation points (for which we have )\n",
    "    int<lower=1> Nimpt;\n",
    "    int<lower=1,upper=N_TxTn> day_impute[Nimpt];\n",
    "    // number of hours recorded within each day\n",
    "    int<lower=1> impt_times_p_day[N_TxTn];\n",
    "\n",
    "    // prior \n",
    "    vector[Nimpt] predicted_mean;\n",
    "    matrix[Nimpt,Nimpt] predicted_cov;\n",
    "    matrix[Nimpt,Nimpt] predicted_cov_chol;\n",
    "\n",
    "    // control soft max hardness\n",
    "    real<lower=0> k_smoothmax;\n",
    "}\n",
    "parameters {\n",
    "    vector[Nimpt] w_uncorr;\n",
    "    real mu;\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[Nimpt] temp_impt;\n",
    "    real Tsmoothmax[N_TxTn];\n",
    "    real Tsmoothmin[N_TxTn];  \n",
    "    temp_impt = mu + predicted_mean + predicted_cov_chol*w_uncorr;\n",
    "    {\n",
    "        int istart;\n",
    "        istart = 1;\n",
    "        for (i in 1:N_TxTn){\n",
    "            int ntimes;\n",
    "            ntimes = impt_times_p_day[i];\n",
    "            Tsoftmin[i] = smoothmin(segment(temp_impt,istart,ntimes), \n",
    "                                    k_smoothmax, \n",
    "                                    k_smoothmax*Tn[i]);\n",
    "            Tsoftmax[i] = smoothmax(segment(temp_impt,istart,ntimes), \n",
    "                                    k_smoothmax,\n",
    "                                    k_smoothmax*Tx[i]);\n",
    "            istart = istart + ntimes;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "model {\n",
    "    w_uncorr ~ normal(0,1);\n",
    "    mu ~ normal(0, 100.0);\n",
    "    Tn ~ normal(Tsmoothmin, 0.1);\n",
    "    Tx ~ normal(Tsmoothmax, 0.1);\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": false,
   "bibliofile": "ref.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 5,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "246px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "987px",
    "left": "0px",
    "right": "auto",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
